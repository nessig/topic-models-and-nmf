{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling and NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level motivation:\n",
    "Given a gigantic corpus of documents (e.g., 300,000 New York Times articles), how would you come up with a set of topics that could be mixed togeather to explain each document in your corpus? Maybe one document is a mixture of 70% about politics and 30% about health, but another document is 20% about politics, 30% about economics, and 50% about cooking. Being able to discover these topics could give deep insight into the existece of fundemental groupings within your data. Additionally, you could quickly create breif and high-level summaries of incomming (possibly large) documents by computing their topic mixtures. **Topic Modeling** is a method of automatic data comprehension and classification whereby latent topics are discovered within a corpus of documents, and used to label existing documents as well as new documents. Topic models are not restricted to news article type documents; models can be constructed from \"documents\" consisting of anything from web pages, to images, to ratings, to genetic sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our topic model\n",
    "In order to learn the topic structure, we must assume the existence of a topic structure from which our documents were generated. Therefore we let $V$ be the size of our vocabulary and let $K$ be the number of topics. We also assume a prior distribution $\\tau$ from which we generate topic distributions for each document, as well as a multinomial distribution $A_k$ over the vocabulary associated with each topic $k$. A document is represented as a vector of word frequencies (bag-of-words model). We model a document as a convex combination (or mixture) of $K$ topics, and topics as distributions over a vocabulary of $V$ words (i.e., topic vectors of word frequencies).   \n",
    "\n",
    "In this *mixture model* a document $d$ is generated by first selecting its distribution of topics (e.g., 80% politics, 20% economics, 0% cooking), which we denote $W_d \\sim \\tau.$ Then, for each word position $i$ in the document we chose a topic for that position $z_i \\sim W_i,$ and finally a word from that topic $w_i \\sim A_{z_i}.$  \n",
    "\n",
    "We may represent each word-topic distribution as a column $A_k$ in a $V \\times K$ word-topic matrix $A$, and similarly define the $K \\times N$ topic-document matrix in terms of the randomly generated topic distributions $W_d$ of our $N$ documents. In this representation we can interpret the $V$ dimensional vector $Av$, where $v \\in \\mathbb{R}^K \\text{ and } \\sum_i v_i = 1$, to be the distribution of word frequencies for a document with topic weights $v$. Taking this product with $A$ for an infinite number of documents $W_d$ gives us the $V \\times N$ matrix $M$ that encodes the expected word frequencies for $N$ documents.\n",
    "\n",
    "For particular topic models like Latent Dirichlet Allocation (LDA) where the prior disribution $\\tau$ over the topic distribution of a document is a Dirichlet distribution, we would like to learn the model's parameters. So, the learning problem for topic modeling is: given topic model how do we actually go about learning the matrix of word-topic distributions $A$ and the paramaters of $\\tau$?  \n",
    "\n",
    "Because MLE is NP-hard for more than one topic, typically people use MCMC algorithms to approximately infer the parameters (e.g., Gibbs sampling). The algorithm described in **Arora et. al. [2012b]** is a simple and highly performant alternative to methods like Gibbs sampling, and will be the focus of this notebook (along with descriptions of the algorithms implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arora's topic inference modeling algorithm\n",
    "In **Arora et. al. [2012a]** the authors prove there is a poly-time algorithm that learns the parameters of any topic model under the assumption that the word-topic matrix $A$ is *seperable*.\n",
    "\n",
    "### Seperability assumption\n",
    "A word-topic matrix $A$ is said to be $p$-seperable if there exists a $p > 0$ such that for all topics $k \\in [K]$ there exists a word $i$ that satisfies $A[i,k] > p$ and $A[i,k'] = 0$ for $k' \\neq k.$ These words are called *anchor words*, and are important because if we find one in a document then we know the document must at least partially be about the anchor word's corresponding topic, since the probability of that word being generated by any other topic is zero. An important observation is that if $A$ is sepreable then in the product $AW = M$ the rows of the matrix $W$ appear as scaled rows of $M$. This happens because for any given anchor word $i$ of $A$ corresponding to topic $k$, the row $A[i, :]$ has a single nonzero value $p_k$ at index $k$. Dotting $A[i, :]$ with $W$ to form row $i$ of $M$ we can see only row $W[k, :]$ appears scaled by $p_k$ in $M$, $M[i, :] = p_k * W[k, :].$ Given $M$, we can reconstruct $W$ using the rows of $M$, and then compute $A$ up to scaling.  \n",
    "\n",
    "### Overview of the algorithm\n",
    "The algorithm has two basic steps:  \n",
    "1) **anchor selection**, which finds the anchor words.  \n",
    "2) **recovery**, which recovers the matrix $A$ and the parameters of $\\tau$.  \n",
    "\n",
    "The input to both steps of the algorithm is the $V \\times V$ word-word co-occurance matrix $Q$, which is normalized to sum to $1$ over all its entries.  \n",
    "\n",
    "The `highLevel` function below implements the full algorithm, using helper functions to generate the input $Q$ for the find anchors stepand the recovery step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highLevel(M, params, K):\n",
    "    \"\"\"\n",
    "    Input: word by document sparse matrix of word counts M,\n",
    "            params including tolerance parameter eps > 0 and more,\n",
    "            number of anchors K.\n",
    "    Output: word-topic matrix A, topic-topic matrix R.\n",
    "    \"\"\"\n",
    "    print \"Recovering word-topic matrix and topic-topic matrix\"\n",
    "    print \"parameters are: vocabulary size = %s, number of documents = %s, number of topics = %s, and tolerance = %s\" \\\n",
    "                    % (M.shape[0], M.shape[1], K, params.eps)\n",
    "    candidates = getCandidates(M.tocsr(), params)\n",
    "    print \"calculating word-word co-occurance matrix\"\n",
    "    t0 = time.time()\n",
    "    Q = wordCooccurrence(M)\n",
    "    print \"Q calculated in: \", time.time() - t0\n",
    "    print \"finding anchors\"\n",
    "    t0 = time.time()\n",
    "    anchors = findAnchors(Q, K, params, candidates)\n",
    "    print \"anchors calculated in: \", time.time() - t0\n",
    "    print \"recovering topic-document matrix A\"\n",
    "    Q2 = Q.copy()\n",
    "    t0 = time.time()\n",
    "    A, iterations = recoverL2(Q, anchors, params.eps)\n",
    "    print \"A calculated in: \", time.time() - t0\n",
    "    return A, anchors, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCandidates(M, params):\n",
    "    candidate_anchors = []\n",
    "    for i in xrange(M.shape[0]):\n",
    "        if M[i, :].nnz > params.anchor_thresh:\n",
    "            candidate_anchors.append(i)\n",
    "    return candidate_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a sparse CSC document matrix M (with floating point entries),\n",
    "# computes the word-word correlation matrix Q\n",
    "def wordCooccurrence(M):\n",
    "    vocabSize = M.shape[0]\n",
    "    numdocs = M.shape[1]\n",
    "    \n",
    "    diag_M = np.zeros(vocabSize)\n",
    "\n",
    "    for d in xrange(M.indptr.size - 1):\n",
    "        \n",
    "        # start and end indices for document d\n",
    "        start = M.indptr[d]\n",
    "        end = M.indptr[d + 1]\n",
    "        \n",
    "        nd = np.sum(M.data[start:end])\n",
    "        row_indices = M.indices[start:end]\n",
    "        \n",
    "        if nd*(nd-1) != 0:\n",
    "            diag_M[row_indices] += M.data[start:end]/(nd*(nd-1))\n",
    "            M.data[start:end] = M.data[start:end]/math.sqrt(nd*(nd-1))\n",
    "    \n",
    "    \n",
    "    Q = M*M.transpose()/numdocs\n",
    "    Q = Q.todense()\n",
    "    Q = np.array(Q, copy=False)\n",
    "\n",
    "    diag_M = diag_M/numdocs\n",
    "    Q = Q - np.diag(diag_M)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `wordCooccurance` we loop through each document to calculate `diag_M`, which is incremented by each documents word counts scaled by $n_d (n_d - 1)$, where $wpd$ is the total number of words in that document. After incrementing `diag_M` we then scale each documents counts by $\\sqrt{n_d (n_d - 1)}$. At the end of the loop on a document $d$ we could form the matrix `M.data[start:end] * M.data[start:end].T - np.diag(diag_M)`, which is equal to $\\frac{1}{n_d(n_d - 1)} \\sum_{i,j \\in [n_d], i \\neq j} e_{z_{d,i}} e_{z_{d,j}}^T$ where $z_{d,i}$ denotes the word in document $d$ at position $i$. Since the expected value of all terms $e_{z_{d,i}} e_{z_{d,j}}^T$ is just $(AW_d)(AW_d)^T$, we have that in expectation `M.data[start:end] * M.data[start:end].T - np.diag(diag_M)` is equal to $\\frac{1}{n_d(n_d - 1)}( \\mathbb{E}[M_dM_d^T] - \\mathbb{E}[\\mathrm{diag}(M_d)]) = \\frac{1}{n_d(n_d - 1)}( \\mathbb{E}[M_d]\\mathbb{E}[M_d]^T + \\mathrm{Cov}(M_d) - \\mathrm{diag}(\\mathbb{E}[M_d])) = A W_d W_d^T A^T$, conditioned on $W_d$. We calculate $Q$ by subtracting the diagonal matrix of our running scaled word counts from the appropiately scaled `M.data[start:end] * M.data[start:end].T` (cancels out the diagonal term in the covariance matrix, thereby making it the unbiased estimator we were looking for) and dividing by number of documents, which has the expectation  $\\frac{1}{N} \\sum_{d=1}^N (AW_d)(AW_d)^T$ conditioned on the $W_d$'s. Later on we will show how performing inference on the word-word co-occurance statistics dramatically speeds up our algorithm, allowing us to only need to pass through our corpus once when calculating $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given infinite documents, the convex hull of the rows of $\\bar{Q}$ will be a simplex with vertices corresponding to anchor words. Unfortunately, we don't have an infinite supply of documents, therefore the best we can do is approximate this set of vertices. Given $V$ points $d_1, \\dots, d_V$  that are each a perturbation of $a_1, \\dots, a_V$ whose convex hull forms a simplex $P$, our goal is to approximate the vertices of $P$.  \n",
    "\n",
    "**Main idea of algorithm:** on each iteration the algorithm finds the furthest point from the subspace spanned by the set of current \"anchors\" $S$. If you already found a few vertices that are each close to different anchor words, you want to look for new anchor words as far away from $\\mathbb{span}(S)$ as possible, since anchor words don't lie within  the span of any other vertices.  \n",
    "\n",
    "This algorithm also utilized dimentionality reduction through a random subspace projection (Achlioptas 2003). The stabalized Gram-Schmidt process is responsible for orthonormalizing the set of current anchors, against whose span all distances are being compared (evidently Gram-Schmidt isn't numerically stable and often results in nonorthogonality!).\n",
    "\n",
    "### findAnchors algorithm\n",
    "Input: $V$ points in $\\mathbb{R}^V$ almost in a simplex (corresponding to $\\bar{Q}$'s rows) with $K$ vertices and $\\epsilon > 0.$  \n",
    "Output: $K$ points that are close to the vertices of the simplex (i.e., approximate anchor words).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnchors(Q, K, params, candidates):\n",
    "    # Random number generator for generating dimension reduction\n",
    "    prng_W = RandomState(params.seed)\n",
    "\n",
    "    new_dim = params.new_dim\n",
    "\n",
    "    # row normalize Q\n",
    "    row_sums = Q.sum(1)\n",
    "    for i in xrange(len(Q[:, 0])):\n",
    "        Q[i, :] = Q[i, :]/float(row_sums[i])\n",
    "\n",
    "    # Reduced dimension random projection method for recovering anchor words\n",
    "    Q_red = Random_Projection(Q.T, new_dim, prng_W)\n",
    "    Q_red = Q_red.T\n",
    "    (anchors, anchor_indices) = Projection_Find(Q_red, K, candidates)\n",
    "\n",
    "    # restore the original Q\n",
    "    for i in xrange(len(Q[:, 0])):\n",
    "        Q[i, :] = Q[i, :]*float(row_sums[i])\n",
    "\n",
    "    return anchor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Projection_Find(M_orig, r, candidates):\n",
    "\n",
    "    n = M_orig[:, 0].size\n",
    "    dim = M_orig[0, :].size\n",
    "\n",
    "    M = M_orig.copy()\n",
    "    \n",
    "    # stored recovered anchor words\n",
    "    anchor_words = np.zeros((r, dim))\n",
    "    anchor_indices = np.zeros(r, dtype=np.int)\n",
    "\n",
    "    # store the basis vectors of the subspace spanned by the anchor word vectors\n",
    "    basis = np.zeros((r-1, dim))\n",
    "\n",
    "\n",
    "    # find the farthest point p1 from the origin\n",
    "    max_dist = 0\n",
    "    #for i in range(0, n):\n",
    "    for i in candidates:\n",
    "        dist = np.dot(M[i], M[i])\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            anchor_words[0] = M_orig[i]\n",
    "            anchor_indices[0] = i\n",
    "\n",
    "    # let p1 be the origin of our coordinate system\n",
    "    #for i in range(0, n):\n",
    "    for i in candidates:\n",
    "        M[i] = M[i] - anchor_words[0]\n",
    "\n",
    "\n",
    "    # find the farthest point from p1\n",
    "    max_dist = 0\n",
    "    for i in candidates:\n",
    "        dist = np.dot(M[i], M[i])\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            anchor_words[1] = M_orig[i]\n",
    "            anchor_indices[1] = i\n",
    "            basis[0] = M[i]/np.sqrt(np.dot(M[i], M[i]))\n",
    "\n",
    "\n",
    "    # stabilized gram-schmidt which finds new anchor words to expand our subspace\n",
    "    for j in range(1, r - 1):\n",
    "\n",
    "        # project all the points onto our basis and find the farthest point\n",
    "        max_dist = 0\n",
    "        #for i in range(0, n):\n",
    "        for i in candidates:\n",
    "            M[i] = M[i] - np.dot(M[i], basis[j-1])*basis[j-1]\n",
    "            dist = np.dot(M[i], M[i])\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                anchor_words[j + 1] = M_orig[i]\n",
    "                anchor_indices[j + 1] = i\n",
    "                basis[j] = M[i]/np.sqrt(np.dot(M[i], M[i])) \n",
    "                \n",
    "    # convert numpy array to python list\n",
    "    anchor_indices_list = []\n",
    "    for i in range(r):\n",
    "        anchor_indices_list.append(anchor_indices[i])\n",
    "    \n",
    "    return (anchor_words, anchor_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project the columns of the matrix M into the\n",
    "# lower dimension new_dim using Achlioptas 2003 method\n",
    "def Random_Projection(M, new_dim, prng):\n",
    "    # transformer = random_projection.SparseRandomProjection(new_dim)\n",
    "    # M_red = transformer.fit_transform(M)\n",
    "    old_dim = M[:, 0].size\n",
    "    p = np.array([1./6, 2./3, 1./6])\n",
    "    c = np.cumsum(p)\n",
    "    randdoubles = prng.random_sample(new_dim*old_dim)\n",
    "    R = np.searchsorted(c, randdoubles)\n",
    "    R = math.sqrt(3)*(R - 1)\n",
    "    R = np.reshape(R, (new_dim, old_dim))\n",
    "    \n",
    "    M_red = np.dot(R, M)\n",
    "    return M_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recoverL2 algorithm\n",
    "For two words in a document $w_1,w_2$ and their topic assignments $z_1,z_1$ we have the following identities:\n",
    "1. $A[i,k] = p(w_1 = i \\vert z_1 = k)$  \n",
    "2. $Q[i,j] = p(w_1 = i, w_2 = j)$  \n",
    "3. $\\bar{Q}[i,j] = p(w_2 = j \\vert w_1 = i)$  \n",
    "\n",
    "Let $S$ be the set of anchor word indexes. The convex hull formed by the rows of $\\bar{Q}$ indexed by elements of $S$ contains all other rows of $\\bar{Q}$. To see this, note that for anchor words $s_k$, we can show $$\\bar{Q}[s_k,j] = p(w_2 = j \\vert z_1 = k).$$ For all other words $i$, we also have $$\\bar{Q}[i,j] = \\sum_k p(z_1 = k \\vert w_1 = i) p(w_2 = j \\vert z_1 = k).$$ If we denote $p(z_1 = k \\vert w_1 = i)$ as the coefficent $\\alpha_{i,k}$ we can rewrite the above expression more compactly as $\\bar{Q}[i,j] = \\sum_k \\alpha[i,k] \\bar{Q}[s_k,j].$ Since $\\alpha$'s rows consist of non-negative probabilities that sum to one, we have that any row of $\\bar{Q}$ lies in the convex hull of the rows corresponding to the anchor words. The mixing weights of these convex combinations give us $p(z_1 \\vert w_1 = i).$ Using this togeather with $p(w_1 = i)$, we can recover $A$ by applying Bayes' rule: $$A[i,k] = p(w_1 \\vert z_1 = k) = \\frac{p(z_1 \\vert w_1 = i) p(w_1 = i)}{\\sum_{i'} p(z_1 = k | w_1 = i') p(w_1 = i')}.$$ We calculate $p(w_1 = i)$ in terms of $Q$ by noting the following equation $$\\sum_j Q[i,j] = \\sum_j p(w_1 = i, w_2 = j) = p(w_1 = i).$$ With all the terms finally computed, we recover $A$ via the application of Bayes' theorem above.  \n",
    "\n",
    "For each row of the empirical row normalized co-occurance matrix $\\hat{Q}$, `recoverL2` finds the coefficents $p(z_1 \\vert w_1 = i)$ that best reconstruct the row as a convex combination of rows corresponding to anchor words using the exponentiated gradient algorithm `quadSolveExpGrad`. Then by applying Bayes' theorem as before, `recoverL2` can recover $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recoverL2(Q, anchors, eps=10**(-7)):\n",
    "    \"\"\"\n",
    "    Input: Matrix Q, Set of words S, tolerance parameter eps\n",
    "    Output: Matrices A,R\n",
    "    \"\"\"\n",
    "    Qold = np.copy(Q)\n",
    "    V = Q.shape[0]\n",
    "    K = len(anchors)\n",
    "    A = np.matrix(np.zeros((V,K)))\n",
    "    iterations = []\n",
    "    \n",
    "    # Store the normalization constnts P_w = Q*ones\n",
    "    P_w = np.matrix(np.diag(np.dot(Q, np.ones(V))))\n",
    "    for v in xrange(V):\n",
    "        if np.isnan(P_w[v,v]):\n",
    "            P_w[v,v] = 10**(-16)\n",
    "    \n",
    "    #normalize the rows of Q_prime\n",
    "    for v in xrange(V):\n",
    "        Q[v,:] = Q[v,:]/Q[v,:].sum()\n",
    "            \n",
    "    X = Q[anchors, :]\n",
    "    XXT = np.dot(X, X.transpose())\n",
    "\n",
    "    for i in xrange(V):\n",
    "        y = Q[i, :]\n",
    "        alpha, it = recover(y,X,i,anchors,XXT,eps)\n",
    "        A[i, :] = alpha\n",
    "        iterations.append(it)\n",
    "    \n",
    "    #rescale A matrix\n",
    "    #Bayes rule says P(w|z) proportional to P(z|w)P(w)\n",
    "\n",
    "    A = P_w * A\n",
    "\n",
    "    #normalize columns of A. This is the normalization constant P(z)\n",
    "    colsums = A.sum(0)\n",
    "\n",
    "    for k in xrange(K):\n",
    "        A[:, k] = A[:, k]/A[:,k].sum()\n",
    "    \n",
    "    A = np.array(A)\n",
    "#     R = np.dot(np.linalg.pinv(A), np.dot(Qold, np.linalg.pinv(A).T))\n",
    "#     R = np.dot(A.conj().T, np.dot(Qold, A.conj()))\n",
    "\n",
    "    return A, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recover(y,x,i,anchors,XXT,eps):\n",
    "    K = len(anchors)\n",
    "    alpha = np.zeros(K)\n",
    "    gap = None\n",
    "    if i in anchors:\n",
    "        alpha[anchors.index(i)] = 1\n",
    "        it = -1\n",
    "        dist = 0\n",
    "        stepsize = 0\n",
    "\n",
    "    else:\n",
    "        # alpha, it, dist, stepsize, gap = quadSolveExpGrad(y, x, eps, None, XXT)\n",
    "        alpha, it = quadSolveExpGrad(y, x, eps, None, XXT)\n",
    "        if np.isnan(alpha).any():\n",
    "                alpha = np.ones(K)/K\n",
    "\n",
    "    return alpha, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadSolveExpGrad(y, X, eps, alpha=None, XX=None): \n",
    "    \"\"\"Exponentiated gradient method\n",
    "    Input: vector y, matrix X, tolerance parameter eps\n",
    "    non-negative normalized vector alpha close to alpha'\n",
    "    that minimizes the L2 distance between y and X*alpha\n",
    "    finds alpha within eps of the alpha' minimizing the objective function obj(y, Xa) = aXXa - 2*aXY + YY,\n",
    "    where aXXA = dot(dot(alpha, dot(x, x.transpose())), alpha.transpose())\n",
    "    and aXY and YY are defined similarly.\n",
    "    Output: non-negative normalized vector alpha close to alpha' that minimizes the L2 distance between y and X*alpha,\n",
    "    number of iterations it,\n",
    "    final value of the objective function new_obj,\n",
    "    amount to change alpha by stepsize,\n",
    "    closeness to minimal alpha solution gap\"\"\"\n",
    "    \n",
    "    c1 = 10**(-4)\n",
    "    c2 = 0.75\n",
    "    if XX is None:\n",
    "        print 'making XXT'\n",
    "        XX = np.dot(X, X.transpose())\n",
    "\n",
    "    XY = np.dot(X, y)\n",
    "    YY = float(np.dot(y, y))\n",
    "\n",
    "\n",
    "    (K,n) = X.shape\n",
    "#     if alpha is None:\n",
    "#         alpha = np.ones(K)/K\n",
    "    alpha = np.ones(K)/K\n",
    "\n",
    "    old_alpha = np.copy(alpha)\n",
    "    log_alpha = np.log(alpha)\n",
    "    old_log_alpha = np.copy(log_alpha)\n",
    "\n",
    "    it = 1 \n",
    "    aXX = np.dot(alpha, XX)\n",
    "    aXY = float(np.dot(alpha, XY))\n",
    "    aXXa = float(np.dot(aXX, alpha.transpose()))\n",
    "\n",
    "    grad = 2*(aXX-XY)\n",
    "    new_obj = aXXa - 2*aXY + YY\n",
    "\n",
    "    old_grad = np.copy(grad)\n",
    "\n",
    "    stepsize = 1\n",
    "    repeat = False\n",
    "    decreased = False\n",
    "    gap = float('inf')\n",
    "\n",
    "    while 1:\n",
    "        eta = stepsize\n",
    "        old_obj = new_obj\n",
    "        old_alpha = np.copy(alpha)\n",
    "        old_log_alpha = np.copy(log_alpha)\n",
    "\n",
    "        if new_obj == 0 or stepsize == 0:\n",
    "            break\n",
    "\n",
    "#         if it % 1000 == 0:\n",
    "#             print \"\\titer\", it, new_obj, gap, stepsize\n",
    "        it += 1\n",
    "        #update\n",
    "        log_alpha -= eta*grad\n",
    "        #normalize\n",
    "        log_alpha -= logsum_exp(log_alpha)\n",
    "        #compute new objective\n",
    "        alpha = np.exp(log_alpha)\n",
    "\n",
    "        aXX = np.dot(alpha, XX)\n",
    "        aXY = float(np.dot(alpha, XY))\n",
    "        aXXa = float(np.dot(aXX, alpha.transpose()))\n",
    "\n",
    "        old_obj = new_obj\n",
    "        new_obj = aXXa - 2*aXY + YY\n",
    "#         old_obj, new_obj = new_obj, aXXa - 2 * aXY + YY\n",
    "        if not new_obj <= old_obj + c1*stepsize*np.dot(grad, alpha - old_alpha): #sufficient decrease\n",
    "            stepsize /= 2.0 #reduce stepsize\n",
    "            alpha = old_alpha \n",
    "            log_alpha = old_log_alpha\n",
    "            new_obj = old_obj\n",
    "            repeat = True\n",
    "            decreased = True\n",
    "            continue\n",
    "\n",
    "        #compute the new gradient\n",
    "        old_grad = np.copy(grad)\n",
    "        grad = 2*(aXX-XY)\n",
    "#         old_obj, new_obj = new_obj, aXXa - 2 * aXY + YY\n",
    "        \n",
    "        if (not np.dot(grad, alpha - old_alpha) >= c2*np.dot(old_grad, alpha-old_alpha)) and (not decreased): #curvature\n",
    "            stepsize *= 2.0 #increase stepsize\n",
    "            alpha = old_alpha\n",
    "            log_alpha = old_log_alpha\n",
    "            grad = old_grad\n",
    "            new_obj = old_obj\n",
    "            repeat = True\n",
    "            continue\n",
    "\n",
    "        decreased = False\n",
    "\n",
    "        lam = np.copy(grad)\n",
    "        lam -= lam.min()\n",
    "        \n",
    "        gap = np.dot(alpha, lam)\n",
    "        convergence = gap\n",
    "        if (convergence < eps):\n",
    "            break\n",
    "\n",
    "    return alpha, it #, it, new_obj, stepsize, gap\n",
    "\n",
    "def logsum_exp(y):\n",
    "    m = y.max()\n",
    "    return m + np.log((np.exp(y - m)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import wget\n",
    "import gzip\n",
    "#import scipy\n",
    "# from sklearn import random_projection\n",
    "import time\n",
    "import math\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# matplotlib.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have any datasets for topic modeling, here are a couple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nips.txt\"\n",
    "docwordUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nips.txt.gz\"\n",
    "wget.download(vocabUrl)\n",
    "wget.download(docwordUrl)\n",
    "\n",
    "# NY Times articles dataset (~300,000 documents, ~100,00 vocabulary, ~100,000,000 words)\n",
    "# vocabUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nytimes.txt\"\n",
    "# docwordUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nytimes.txt.gz\"\n",
    "# wget.download(vocabUrl)\n",
    "# wget.download(docwordUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the dataset into a sparse array below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make nips (or new york times) dataset!\n",
    "input_matrix = \"datasets/nips/docword.nips.txt.gz\"\n",
    "full_vocab = \"datasets/nips/vocab.nips.txt\"\n",
    "# input_matrix = \"datasets/nyt/docword.nytimes.txt\"\n",
    "# full_vocab = \"datasets/nyt/vocab.nytimes.txt\"\n",
    "\n",
    "firstNdocs = 'all'\n",
    "# output_matrix_name = \"datasets/nyt/M_nytimes.%s.mat\" % (firstNdocs) # \"M_nips.full_docs.mat\"\n",
    "output_matrix_name = \"datasets/nips/M_nips.%s.mat\" % (firstNdocs)\n",
    "\n",
    "print \"opening file\"\n",
    "\n",
    "with gzip.open(input_matrix, 'r') as infile:\n",
    "# with open(input_matrix, 'r') as infile:\n",
    "    num_docs = int(infile.readline())\n",
    "    num_words = int(infile.readline())\n",
    "    nnz = int(infile.readline())\n",
    "\n",
    "    print \"number of docs = %s, number of words = %s, number nonzero = %s\" % (num_docs, num_words, nnz)\n",
    "\n",
    "    data = []        # counts\n",
    "    row = []         # row (document) indices\n",
    "    col = []         # column (word) indices\n",
    "\n",
    "    print \"constructing word-document matrix\"\n",
    "    t0 = time.time()\n",
    "    if firstNdocs == 'all':\n",
    "        for i, l in enumerate(infile):\n",
    "            d, w, v = (int(x) for x in l.split())\n",
    "            if not i % 100000:\n",
    "                print i, time.time() - t0, d, w, v\n",
    "                t0 = time.time()\n",
    "            col.append(d-1)\n",
    "            row.append(w-1)\n",
    "            data.append(v)\n",
    "    else:\n",
    "        for i, l in enumerate(infile):\n",
    "            d, w, v = (int(x) for x in l.split())\n",
    "            if not i % 100000:\n",
    "                print i, time.time() - t0, d, w, v\n",
    "                t0 = time.time()\n",
    "\n",
    "            if d == firstNdocs + 1:\n",
    "                break\n",
    "            col.append(d-1)\n",
    "            row.append(w-1)\n",
    "            data.append(v)\n",
    "            # M[w-1, d-1] = v\n",
    "    M = scipy.sparse.csr_matrix((data, (row, col)))\n",
    "\n",
    "scipy.io.savemat(output_matrix_name, {'M' : M}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncateData(matrixName, full_vocab, numDocs=None, cutoff = 1000):\n",
    "    M = scipy.io.loadmat(matrixName)['M']\n",
    "    output_vocab = full_vocab + \".trunc\"\n",
    "    \n",
    "#     randIndices = np.random.choice(range(M.shape[1]), numDocs, replace=False)\n",
    "\n",
    "    print \"old num words = %s, old num docs = %s\" % M.shape\n",
    "\n",
    "    table = dict()\n",
    "    numwords = 0\n",
    "    with open(full_vocab, 'r') as f:\n",
    "        for line in f:\n",
    "            table[line.rstrip()] = numwords\n",
    "            numwords += 1\n",
    "\n",
    "    remove_word = [False]*numwords\n",
    "\n",
    "    # Read in the stopwords\n",
    "    with open('datasets/stopwords.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.rstrip() in table:\n",
    "                remove_word[table[line.rstrip()]] = True\n",
    "\n",
    "#     M = M[:,randIndices].tocsr() # M.tocsr()\n",
    "#     M = M.tocsr()\n",
    "    if numDocs:\n",
    "        randIndices = np.random.choice(range(M.shape[1]), numDocs, replace=False)\n",
    "        M = M[:,randIndices].tocsr()\n",
    "    else:\n",
    "        M = M.tocsr()\n",
    "    \n",
    "    new_indptr = np.zeros(M.indptr.shape[0], dtype=np.int32)\n",
    "    new_indices = np.zeros(M.indices.shape[0], dtype=np.int32)\n",
    "    new_data = np.zeros(M.data.shape[0], dtype=np.float64)\n",
    "\n",
    "    indptr_counter = 1\n",
    "    data_counter = 0\n",
    "\n",
    "    for i in xrange(M.indptr.size - 1):\n",
    "\n",
    "        # if this is not a stopword\n",
    "        if not remove_word[i]:\n",
    "\n",
    "            # start and end indices for row i\n",
    "            start = M.indptr[i]\n",
    "            end = M.indptr[i + 1]\n",
    "\n",
    "            # if number of distinct documents that this word appears in is >= cutoff\n",
    "            if (end - start) >= cutoff:\n",
    "                new_indptr[indptr_counter] = new_indptr[indptr_counter-1] + end - start\n",
    "                new_data[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.data[start:end]\n",
    "                new_indices[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.indices[start:end]\n",
    "                indptr_counter += 1\n",
    "            else:\n",
    "                remove_word[i] = True\n",
    "\n",
    "    new_indptr = new_indptr[0:indptr_counter]\n",
    "    new_indices = new_indices[0:new_indptr[indptr_counter-1]]\n",
    "    new_data = new_data[0:new_indptr[indptr_counter-1]]\n",
    "\n",
    "    M = scipy.sparse.csr_matrix((new_data, new_indices, new_indptr))\n",
    "    Mtrunc = M.tocsc()\n",
    "\n",
    "    print \"new num words = %s, new num docs = %s\" % M.shape\n",
    "\n",
    "    # Output the new vocabulary\n",
    "    output = open(output_vocab, 'w')\n",
    "    row = 0\n",
    "    with open(full_vocab, 'r') as f:\n",
    "        for line in f:\n",
    "            if not remove_word[row]:\n",
    "                output.write(line)\n",
    "            row += 1\n",
    "    output.close()\n",
    "    \n",
    "    return Mtrunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    def __init__(self, seed=int(time.time()), top_words=10, new_dim=1000, eps=10e-7, anchor_thresh=100):\n",
    "        self.seed = seed\n",
    "        self.top_words = top_words\n",
    "        self.new_dim = new_dim\n",
    "        self.eps = eps\n",
    "        self.anchor_thresh = anchor_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old num words = 102660, old num docs = 300000\n",
      "new num words = 8367, new num docs = 50000\n"
     ]
    }
   ],
   "source": [
    "input_matrix = \"datasets/nips/M_nips.all.mat\"\n",
    "full_vocab = \"datasets/nips/vocab.nips.txt\"\n",
    "numDocs = 1500\n",
    "thresh = 100\n",
    "cutoff = 50\n",
    "\n",
    "# input_matrix = \"datasets/nyt/M_nytimes.all.mat\"\n",
    "# full_vocab = \"datasets/nyt/vocab.nytimes.txt\"\n",
    "# numDocs = 50000\n",
    "# thresh = 500 # Anchor words must be in more than this number of documents\n",
    "# cutoff = 200 # Words must be in at least this number of documents, otherwise they are pruned.\n",
    "\n",
    "Mtrunc = truncateData(input_matrix, full_vocab, numDocs=numDocs, cutoff=cutoff)\n",
    "params = Params(seed=100, anchor_thresh=thresh, top_words=6, eps=1e-5)\n",
    "params.dictionary_file = full_vocab + \".trunc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole algorithm in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering word-topic matrix and topic-topic matrix\n",
      "parameters are: vocabulary size = 8367, number of documents = 50000, number of topics = 100, and tolerance = 1e-05\n",
      "calculating word-word co-occurance matrix\n",
      "Q calculated in:  36.6643409729\n",
      "finding anchors\n",
      "anchors calculated in:  8.71388411522\n",
      "recovering topic-document matrix A\n",
      "A calculated in:  39.8705518246\n",
      "88.4953508377\n"
     ]
    }
   ],
   "source": [
    "K = 100 # Number of topics to learn\n",
    "t0 = time.time()\n",
    "A, anchors, Q = highLevel(Mtrunc.copy(), params, K)\n",
    "print time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the words most associated with each topic in order of descending importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each topic's anchor word followed by a list of keywords in order of descending importance.\n",
      "\n",
      "advisory : advisory premature advise error wines client nyt zzz_islamabad fort held \n",
      "zzz_karl_horwitz : zzz_karl_horwitz financial lifestyle zzz_new_york zzz_americas mail general sector miles zzz_los_angeles \n",
      "tonight : tonight zzz_boston_globe zzz_x_x_x spot zzz_new_york zzz_fla light court zzz_chicago development \n",
      "copy : copy fall zzz_diane question expected answer version killed schedule zzz_new_york \n",
      "test : test export function mark thread housed media student check paragraph \n",
      "file : file zzz_boston_globe zzz_new_york spot sport zzz_fla zzz_los_angeles notebook internet zzz_calif \n",
      "solo : con dice solo son mayor era director sin sector embargo \n",
      "zzz_portland : zzz_portland coach assistant season team player job head coaching coaches \n",
      "publication : publication premature wines nyt zzz_islamabad greenhouse zzz_lee substitute risen advise \n",
      "wire : wire mandatory finance running today zzz_x_x_x produced tomorrow export function \n",
      "shares : million shares company offering public debt billion stock initial expected \n",
      "fax : fax financial mail zzz_new_york lifestyle zzz_americas general zzz_los_angeles miles solar \n",
      "zzz_paris : zzz_paris financial lifestyle zzz_new_york mail zzz_americas general sector miles zzz_los_angeles \n",
      "dates : sales dates major economic claim home scheduled weekly listed order \n",
      "send : send client error point advise word held telegram worth fort \n",
      "inning : run inning hit game home pitch ball lead left homer \n",
      "guard : guard team game player season play word close schedule games \n",
      "zzz_tom_oder : information zzz_eastern sport daily commentary question business separate marked today \n",
      "zzz_yasser_arafat : palestinian zzz_israel zzz_israeli zzz_yasser_arafat peace israeli official leader israelis attack \n",
      "released : released client error word point close telegram worth held law \n",
      "favor : favor financial lifestyle zzz_new_york mail general zzz_americas zzz_los_angeles miles zzz_peru \n",
      "kill : kill today function produced running water tomorrow part film movie \n",
      "par : zzz_tiger_wood shot player round par play tournament tour win won \n",
      "zzz_enron : zzz_enron company companies business firm stock employees executive billion chief \n",
      "zzz_laker : point zzz_laker team game season play zzz_kobe_bryant games player zzz_o_neal \n",
      "pepper : cup minutes add oil tablespoon pepper water sauce teaspoon fat \n",
      "zzz_microsoft : zzz_microsoft company computer software system window companies case business product \n",
      "tables : democratic tables environmental turning room home show restaurant night place \n",
      "touchdown : game yard play season team touchdown quarterback games coach player \n",
      "zzz_nasdaq : percent stock market company companies quarter point analyst investor economy \n",
      "earlier : earlier spot zzz_boston_globe article company stated advise zzz_washington summer zzz_nfl \n",
      "ballot : election ballot vote voter votes zzz_florida court recount official campaign \n",
      "zzz_taliban : zzz_taliban zzz_afghanistan government afghan forces official military zzz_u_s troop war \n",
      "glasses : glasses raise sexy thick frames home room friend book find \n",
      "anthrax : anthrax official mail letter attack office building worker zzz_fbi found \n",
      "lap : race car driver lap racing track win point won run \n",
      "ages : book children ages boy list sales find web author ranking \n",
      "zzz_cnn : zzz_cnn network president executive company business chief executives job cable \n",
      "priest : priest church abuse bishop sexual official victim children ago member \n",
      "newspaper : newspaper question evening fall endit source zzz_calif group part center \n",
      "zzz_medicare : drug percent plan program cost care health bill zzz_medicare benefit \n",
      "org : www site web org sites information online visit telegram mail \n",
      "shower : air rain wind shower storm weather front high water northern \n",
      "breast : women cancer percent patient breast study doctor drug risk found \n",
      "brown : dog corp brown quick jump lazy list home partial water \n",
      "king : king game goal team play season games point player period \n",
      "dedicated : dedicated producer include seek site part million show group member \n",
      "zzz_beijing : zzz_china chinese official government zzz_beijing zzz_united_states zzz_taiwan zzz_u_s leader country \n",
      "toss : survivor series toss interesting meaning set play find word water \n",
      "zzz_al_gore : zzz_al_gore campaign president zzz_bush voter democratic presidential vice poll zzz_clinton \n",
      "zzz_met : zzz_met team season player games braves play game fan manager \n",
      "ranger : ranger season team game player play games manager league guy \n",
      "zzz_black : zzz_black percent black white women book student american history number \n",
      "zzz_dodger : zzz_dodger season games manager start won guy league play win \n",
      "zzz_ucla : team game season point player play coach zzz_ucla games win \n",
      "smoking : fund smoking money scratch accompanying study group found percent million \n",
      "religion : religion error correct religious morning zzz_god group book leader political \n",
      "wine : wine wines flavor zzz_california restaurant fruit red food collection grapes \n",
      "album : music song band album show rock record sound pop group \n",
      "noon : talk wall noon earth upcoming american room home fury hour \n",
      "zzz_cuba : zzz_cuba government zzz_miami zzz_united_states cuban official zzz_u_s father boy family \n",
      "missile : zzz_bush missile system defense administration zzz_clinton official nuclear zzz_united_states zzz_russia \n",
      "zzz_aid : drug zzz_aid million percent government patient countries billion zzz_hiv group \n",
      "medal : team zzz_olympic medal women won games gold win sport zzz_u_s \n",
      "zzz_christmas : zzz_christmas morning throw family home hour book friend holiday room \n",
      "chocolate : chocolate sugar food cream cup flavor egg buy milk butter \n",
      "zzz_cb : show network zzz_cb zzz_nbc television zzz_abc season series night hour \n",
      "zzz_india : zzz_india zzz_pakistan government group zzz_united_states official war country military zzz_indian \n",
      "surplus : tax billion cut zzz_bush government percent economy money spending fund \n",
      "gun : gun law police control bill show officer group shooting weapon \n",
      "zzz_giant : team season player zzz_giant game coach games play guy league \n",
      "referred : article referred governor zzz_new_york campaign zzz_new_hampshire president company defeated political \n",
      "zzz_ford : company car zzz_ford tires sales percent sport vehicles chief market \n",
      "yankees : yankees team season player million zzz_new_york won play baseball win \n",
      "zzz_oscar : film movie director actor play award character movies show zzz_oscar \n",
      "abortion : women abortion law zzz_bush campaign court group republican political bill \n",
      "zzz_russian : zzz_russian zzz_russia official zzz_vladimir_putin russian zzz_moscow government military station officer \n",
      "pension : plan fund company percent money million companies stock pension worker \n",
      "zzz_dick_cheney : zzz_bush president zzz_dick_cheney zzz_white_house administration republican campaign attack zzz_congress presidential \n",
      "defendant : case court lawyer trial law death prosecutor federal judge attorney \n",
      "gay : gay group member sex show political republican lesbian meeting part \n",
      "gallon : prices oil car water gas price fuel gallon cost power \n",
      "stem : research stem human scientist company zzz_bush patient president plant researcher \n",
      "elementary : school student teacher program children high parent public kid education \n",
      "purchases : percent company information companies customer sales market million business consumer \n",
      "flag : flag zzz_american black american attack white country zzz_america member political \n",
      "virus : virus computer program official system disease mail found bird human \n",
      "sanction : government trade official zzz_united_states sanction zzz_u_s zzz_iraq zzz_iran zzz_american administration \n",
      "grandchildren : zzz_new_york family son died home wife children daughter book father \n",
      "airlines : percent flight company passenger security airline business official companies airport \n",
      "horses : race won horse horses winner trainer million win racing stakes \n",
      "laden : laden bin attack terrorist official zzz_u_s zzz_united_states zzz_american zzz_bush terrorism \n",
      "cow : animal cow food disease human zzz_new_york plant found part product \n",
      "auction : million company percent auction site business market companies bill web \n",
      "zzz_mexico : zzz_mexico zzz_vicente_fox director mexican sector miles trade cafe local region \n",
      "telex : syndicate zzz_new_york www telex zzz_canada zzz_asia era http zzz_new_jersey hotel \n",
      "zzz_george_bush : zzz_george_bush president campaign zzz_john_mccain political republican election zzz_texas presidential zzz_republican \n",
      "privacy : web information companies site computer law mail privacy company zzz_internet \n",
      "leather : season women show designer black leather fashion car collection boy \n",
      "zzz_nato : war military zzz_nato government zzz_kosovo zzz_united_states troop leader zzz_u_s official \n"
     ]
    }
   ],
   "source": [
    "print \"Each topic's anchor word followed by a list of keywords in order of descending importance.\\n\"\n",
    "vocab = file(full_vocab + \".trunc\").read().strip().split()\n",
    "for k in xrange(K):\n",
    "    topwords = np.argsort(A[:, k])[-10:][::-1]\n",
    "    print vocab[anchors[k]], ':',\n",
    "    for w in topwords:\n",
    "        print vocab[w],\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the topic-topic correlation matrix to view potential correlations between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x109c67150>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAJeCAYAAAD82dIzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a63dd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "R = np.dot(np.linalg.pinv(A), np.dot(Q, np.linalg.pinv(A.T))) # Computing the topic-topic covariance matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pcolor(R, norm=None, cmap='Blues')\n",
    "plt.title(\"Topic-topic correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the parameters of $\\alpha$ for the topic-document Dirichlet distribution $\\tau.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcAlpha(R):\n",
    "    alphaOverAlpha0 = np.dot(R, np.ones(R.shape[0]))\n",
    "    i = alphaOverAlpha0.argmin()\n",
    "    u, v = R[i,i], alphaOverAlpha0[i]\n",
    "    alpha0 = (1 - (u/v))/((u/v) - v)\n",
    "    alpha = alpha0 * alphaOverAlpha0\n",
    "    return alpha, alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01482729,  0.01684305,  0.04064545,  0.0371535 ,  0.09473873,\n",
       "         0.03836407,  0.16079619,  0.10067844,  0.02738376,  0.06537202,\n",
       "         0.49560479,  0.04703361,  0.03167202,  0.26383021,  0.03998449,\n",
       "         0.53474516,  0.04023191,  0.40552972,  0.75414944,  0.05027042,\n",
       "         0.03312532,  0.17254458,  0.45826832,  0.57504078,  0.69894165,\n",
       "         0.49661029,  0.60142218,  0.37659544,  0.79724495,  1.11137475,\n",
       "         0.25841612,  0.66166829,  0.55120452,  0.37938664,  0.48810023,\n",
       "         0.51435209,  0.54211884,  0.36812331,  0.55316861,  0.19576816,\n",
       "         0.6076717 ,  0.43812427,  0.75566975,  0.58344115,  0.31518189,\n",
       "         0.75048077,  0.39136663,  0.58293235,  0.18629543,  0.52199163,\n",
       "         0.33368786,  0.49258965,  0.52855665,  0.28093796,  0.90778155,\n",
       "         0.40971158,  0.4640747 ,  0.41657754,  1.04263171,  0.24761961,\n",
       "         0.34118148,  0.41079399,  0.48217188,  0.67416518,  0.4514224 ,\n",
       "         0.34667813,  0.78944787,  0.53289068,  0.61997093,  0.64970896,\n",
       "         0.47919272,  0.4773617 ,  0.6837891 ,  0.40872409,  1.37016115,\n",
       "         0.61513208,  0.58168425,  0.70065155,  0.45906463,  1.3696742 ,\n",
       "         0.61091451,  0.76572838,  0.66870645,  1.51043102,  0.60618614,\n",
       "         0.57228746,  0.54505714,  0.48287676,  1.24425684,  1.24597427,\n",
       "         0.60623593,  0.43250997,  0.65849959,  0.92584321,  0.10687744,\n",
       "         0.0902769 ,  0.45024457,  0.99013477,  0.88618195,  0.59413717]),\n",
       " 49.548879569353623)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha, alpha0 = calcAlpha(R)\n",
    "alpha, alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Papers discussed:  \n",
    "**Arora et. al. [2012a]:** Arora, S., Ge, R., and Moitra, A. Learning topic models – going beyond svd. In FOCS, 2012b.  \n",
    "**Arora et. al. [2012b]:** Sanjeev Arora, Rong Ge, Yoni Halpern, David M. Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. 2013. A practical algorithm for topic modeling with provable guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
