{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling and NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level motivation:\n",
    "Given a gigantic corpus of documents (e.g., 300,000 New York Times articles), how would you come up with a set of topics that could be mixed togeather to explain each document in your corpus? Maybe one document is a mixture of 70% about politics and 30% about health, but another document is 20% about politics, 30% about economics, and 50% about cooking. Being able to discover these topics could give deep insight into the existece of fundemental groupings within your data. Additionally, you could quickly create breif and high-level summaries of incomming (possibly large) documents by computing their topic mixtures. **Topic Modeling** is a method of automatic data comprehension and classification whereby latent topics are discovered within a corpus of documents, and used to label existing documents as well as new documents. Topic models are not restricted to news article type documents; models can be constructed from \"documents\" consisting of anything from web pages, to images, to ratings, to genetic sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our topic model\n",
    "In order to learn the topic structure, we must assume the existence of a topic structure from which our documents were generated. Therefore we let $V$ be the size of our vocabulary and let $K$ be the number of topics. We also assume a prior distribution $\\tau$ from which we generate topic distributions for each document, as well as a multinomial distribution $A_k$ over the vocabulary associated with each topic $k$. A document is represented as a vector of word frequencies (bag-of-words model). We model a document as a convex combination (or mixture) of $K$ topics, and topics as distributions over a vocabulary of $V$ words (i.e., topic vectors of word frequencies).   \n",
    "\n",
    "In this *mixture model* a document $d$ is generated by first selecting its distribution of topics (e.g., 80% politics, 20% economics, 0% cooking), which we denote $W_d \\sim \\tau.$ Then, for each word position $i$ in the document we chose a topic for that position $z_i \\sim W_i,$ and finally a word from that topic $w_i \\sim A_{z_i}.$  \n",
    "\n",
    "We may represent each word-topic distribution as a column $A_k$ in a $V \\times K$ word-topic matrix $A$, and similarly define the $K \\times N$ topic-document matrix in terms of the randomly generated topic distributions $W_d$ of our $N$ documents. In this representation we can interpret the $V$ dimensional vector $Av$, where $v \\in \\mathbb{R}^K \\text{ and } \\sum_i v_i = 1$, to be the distribution of word frequencies for a document with topic weights $v$. Taking this product with $A$ for an infinite number of documents $W_d$ gives us the $V \\times N$ matrix $M$ that encodes the expected word frequencies for $N$ documents.\n",
    "\n",
    "For particular topic models like Latent Dirichlet Allocation (LDA) where the prior disribution $\\tau$ over the topic distribution of a document is a Dirichlet distribution, we would like to learn the model's parameters. So, the learning problem for topic modeling is: given topic model how do we actually go about learning the matrix of word-topic distributions $A$ and the paramaters of $\\tau$?  \n",
    "\n",
    "Because MLE is NP-hard for more than one topic, typically people use MCMC algorithms to approximately infer the parameters (e.g., Gibbs sampling). The algorithm described in **Arora et. al. [2012b]** is a simple and highly performant alternative to methods like Gibbs sampling, and will be the focus of this notebook (along with descriptions of the algorithms implementation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arora's topic inference modeling algorithm\n",
    "In **Arora et. al. [2012a]** the authors prove there is a poly-time algorithm that learns the parameters of any topic model under the assumption that the word-topic matrix $A$ is *seperable*.\n",
    "\n",
    "### Seperability assumption\n",
    "A word-topic matrix $A$ is said to be $p$-seperable if there exists a $p > 0$ such that for all topics $k \\in [K]$ there exists a word $i$ that satisfies $A[i,k] > p$ and $A[i,k'] = 0$ for $k' \\neq k.$ These words are called *anchor words*, and are important because if we find one in a document then we know the document must at least partially be about the anchor word's corresponding topic, since the probability of that word being generated by any other topic is zero. An important observation is that if $A$ is sepreable then in the product $AW = M$ the rows of the matrix $W$ appear as scaled rows of $M$. This happens because for any given anchor word $i$ of $A$ corresponding to topic $k$, the row $A[i, :]$ has a single nonzero value $p_k$ at index $k$. Dotting $A[i, :]$ with $W$ to form row $i$ of $M$ we can see only row $W[k, :]$ appears scaled by $p_k$ in $M$, $M[i, :] = p_k * W[k, :].$ Given $M$, we can reconstruct $W$ using the rows of $M$, and then compute $A$ up to scaling.  \n",
    "\n",
    "### Overview of the algorithm\n",
    "The algorithm has two basic steps:  \n",
    "1) **anchor selection**, which finds the anchor words.  \n",
    "2) **recovery**, which recovers the matrix $A$ and the parameters of $\\tau$.  \n",
    "\n",
    "The input to both steps of the algorithm is the $V \\times V$ word-word co-occurance matrix $Q$, which is normalized to sum to $1$ over all its entries.  \n",
    "\n",
    "The `highLevel` function below implements the full algorithm, using helper functions to generate the input $Q$ for the find anchors stepand the recovery step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highLevel(M, params, K):\n",
    "    \"\"\"\n",
    "    Input: word by document sparse matrix of word counts M,\n",
    "            params including tolerance parameter eps > 0 and more,\n",
    "            number of anchors K.\n",
    "    Output: word-topic matrix A, topic-topic matrix R.\n",
    "    \"\"\"\n",
    "    print \"Recovering word-topic matrix and topic-topic matrix\"\n",
    "    print \"parameters are: vocabulary size = %s, number of documents = %s, number of topics = %s, and tolerance = %s\" \\\n",
    "                    % (M.shape[0], M.shape[1], K, params.eps)\n",
    "    candidates = getCandidates(M.tocsr(), params)\n",
    "    print \"calculating word-word co-occurance matrix\"\n",
    "    t0 = time.time()\n",
    "    Q = wordCooccurrence(M)\n",
    "    print \"Q calculated in: \", time.time() - t0\n",
    "    print \"finding anchors\"\n",
    "    t0 = time.time()\n",
    "    anchors = findAnchors(Q, K, params, candidates)\n",
    "    print \"anchors calculated in: \", time.time() - t0\n",
    "    print \"recovering topic-document matrix A\"\n",
    "    Q2 = Q.copy()\n",
    "    t0 = time.time()\n",
    "    A, iterations = recoverL2(Q, anchors, params.eps)\n",
    "    print \"A calculated in: \", time.time() - t0\n",
    "    return A, anchors, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCandidates(M, params):\n",
    "    candidate_anchors = []\n",
    "    for i in xrange(M.shape[0]):\n",
    "        if M[i, :].nnz > params.anchor_thresh:\n",
    "            candidate_anchors.append(i)\n",
    "    return candidate_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a sparse CSC document matrix M (with floating point entries),\n",
    "# computes the word-word correlation matrix Q\n",
    "def wordCooccurrence(M):\n",
    "    vocabSize = M.shape[0]\n",
    "    numdocs = M.shape[1]\n",
    "    \n",
    "    diag_M = np.zeros(vocabSize)\n",
    "\n",
    "    for d in xrange(M.indptr.size - 1):\n",
    "        \n",
    "        # start and end indices for document d\n",
    "        start = M.indptr[d]\n",
    "        end = M.indptr[d + 1]\n",
    "        \n",
    "        nd = np.sum(M.data[start:end])\n",
    "        row_indices = M.indices[start:end]\n",
    "        \n",
    "        if nd*(nd-1) != 0:\n",
    "            diag_M[row_indices] += M.data[start:end]/(nd*(nd-1))\n",
    "            M.data[start:end] = M.data[start:end]/math.sqrt(nd*(nd-1))\n",
    "    \n",
    "    \n",
    "    Q = M*M.transpose()/numdocs\n",
    "    Q = Q.todense()\n",
    "    Q = np.array(Q, copy=False)\n",
    "\n",
    "    diag_M = diag_M/numdocs\n",
    "    Q = Q - np.diag(diag_M)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `wordCooccurance` we loop through each document to calculate `diag_M`, which is incremented by each documents word counts scaled by $n_d (n_d - 1)$, where $wpd$ is the total number of words in that document. After incrementing `diag_M` we then scale each documents counts by $\\sqrt{n_d (n_d - 1)}$. At the end of the loop on a document $d$ we could form the matrix `M.data[start:end] * M.data[start:end].T - np.diag(diag_M)`, which is equal to $\\frac{1}{n_d(n_d - 1)} \\sum_{i,j \\in [n_d], i \\neq j} e_{z_{d,i}} e_{z_{d,j}}^T$ where $z_{d,i}$ denotes the word in document $d$ at position $i$. Since the expected value of all terms $e_{z_{d,i}} e_{z_{d,j}}^T$ is just $(AW_d)(AW_d)^T$, we have that in expectation `M.data[start:end] * M.data[start:end].T - np.diag(diag_M)` is equal to $\\frac{1}{n_d(n_d - 1)}( \\mathbb{E}[M_dM_d^T] - \\mathbb{E}[\\mathrm{diag}(M_d)]) = \\frac{1}{n_d(n_d - 1)}( \\mathbb{E}[M_d]\\mathbb{E}[M_d]^T + \\mathrm{Cov}(M_d) - \\mathrm{diag}(\\mathbb{E}[M_d])) = A W_d W_d^T A^T$, conditioned on $W_d$. We calculate $Q$ by subtracting the diagonal matrix of our running scaled word counts from the appropiately scaled `M.data[start:end] * M.data[start:end].T` (cancels out the diagonal term in the covariance matrix, thereby making it the unbiased estimator we were looking for) and dividing by number of documents, which has the expectation  $\\frac{1}{N} \\sum_{d=1}^N (AW_d)(AW_d)^T$ conditioned on the $W_d$'s. Later on we will show how performing inference on the word-word co-occurance statistics dramatically speeds up our algorithm, allowing us to only need to pass through our corpus once when calculating $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given infinite documents, the convex hull of the rows of $\\bar{Q}$ will be a simplex with vertices corresponding to anchor words. Unfortunately, we don't have an infinite supply of documents, therefore the best we can do is approximate this set of vertices. Given $V$ points $d_1, \\dots, d_V$  that are each a perturbation of $a_1, \\dots, a_V$ whose convex hull forms a simplex $P$, our goal is to approximate the vertices of $P$.  \n",
    "\n",
    "**Main idea of algorithm:** on each iteration the algorithm finds the furthest point from the subspace spanned by the set of current \"anchors\" $S$. If you already found a few vertices that are each close to different anchor words, you want to look for new anchor words as far away from $\\mathbb{span}(S)$ as possible, since anchor words don't lie within  the span of any other vertices.  \n",
    "\n",
    "This algorithm also utilized dimentionality reduction through a random subspace projection (Achlioptas 2003). The stabalized Gram-Schmidt process is responsible for orthonormalizing the set of current anchors, against whose span all distances are being compared (evidently Gram-Schmidt isn't numerically stable and often results in nonorthogonality!).\n",
    "\n",
    "### findAnchors algorithm\n",
    "Input: $V$ points in $\\mathbb{R}^V$ almost in a simplex (corresponding to $\\bar{Q}$'s rows) with $K$ vertices and $\\epsilon > 0.$  \n",
    "Output: $K$ points that are close to the vertices of the simplex (i.e., approximate anchor words).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnchors(Q, K, params, candidates):\n",
    "    # Random number generator for generating dimension reduction\n",
    "    prng_W = RandomState(params.seed)\n",
    "\n",
    "    new_dim = params.new_dim\n",
    "\n",
    "    # row normalize Q\n",
    "    row_sums = Q.sum(1)\n",
    "    for i in xrange(len(Q[:, 0])):\n",
    "        Q[i, :] = Q[i, :]/float(row_sums[i])\n",
    "\n",
    "    # Reduced dimension random projection method for recovering anchor words\n",
    "    Q_red = Random_Projection(Q.T, new_dim, prng_W)\n",
    "    Q_red = Q_red.T\n",
    "    (anchors, anchor_indices) = Projection_Find(Q_red, K, candidates)\n",
    "\n",
    "    # restore the original Q\n",
    "    for i in xrange(len(Q[:, 0])):\n",
    "        Q[i, :] = Q[i, :]*float(row_sums[i])\n",
    "\n",
    "    return anchor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Projection_Find(M_orig, r, candidates):\n",
    "\n",
    "    n = M_orig[:, 0].size\n",
    "    dim = M_orig[0, :].size\n",
    "\n",
    "    M = M_orig.copy()\n",
    "    \n",
    "    # stored recovered anchor words\n",
    "    anchor_words = np.zeros((r, dim))\n",
    "    anchor_indices = np.zeros(r, dtype=np.int)\n",
    "\n",
    "    # store the basis vectors of the subspace spanned by the anchor word vectors\n",
    "    basis = np.zeros((r-1, dim))\n",
    "\n",
    "\n",
    "    # find the farthest point p1 from the origin\n",
    "    max_dist = 0\n",
    "    #for i in range(0, n):\n",
    "    for i in candidates:\n",
    "        dist = np.dot(M[i], M[i])\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            anchor_words[0] = M_orig[i]\n",
    "            anchor_indices[0] = i\n",
    "\n",
    "    # let p1 be the origin of our coordinate system\n",
    "    #for i in range(0, n):\n",
    "    for i in candidates:\n",
    "        M[i] = M[i] - anchor_words[0]\n",
    "\n",
    "\n",
    "    # find the farthest point from p1\n",
    "    max_dist = 0\n",
    "    for i in candidates:\n",
    "        dist = np.dot(M[i], M[i])\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            anchor_words[1] = M_orig[i]\n",
    "            anchor_indices[1] = i\n",
    "            basis[0] = M[i]/np.sqrt(np.dot(M[i], M[i]))\n",
    "\n",
    "\n",
    "    # stabilized gram-schmidt which finds new anchor words to expand our subspace\n",
    "    for j in range(1, r - 1):\n",
    "\n",
    "        # project all the points onto our basis and find the farthest point\n",
    "        max_dist = 0\n",
    "        #for i in range(0, n):\n",
    "        for i in candidates:\n",
    "            M[i] = M[i] - np.dot(M[i], basis[j-1])*basis[j-1]\n",
    "            dist = np.dot(M[i], M[i])\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                anchor_words[j + 1] = M_orig[i]\n",
    "                anchor_indices[j + 1] = i\n",
    "                basis[j] = M[i]/np.sqrt(np.dot(M[i], M[i])) \n",
    "                \n",
    "    # convert numpy array to python list\n",
    "    anchor_indices_list = []\n",
    "    for i in range(r):\n",
    "        anchor_indices_list.append(anchor_indices[i])\n",
    "    \n",
    "    return (anchor_words, anchor_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project the columns of the matrix M into the\n",
    "# lower dimension new_dim using Achlioptas 2003 method\n",
    "def Random_Projection(M, new_dim, prng):\n",
    "    # transformer = random_projection.SparseRandomProjection(new_dim)\n",
    "    # M_red = transformer.fit_transform(M)\n",
    "    old_dim = M[:, 0].size\n",
    "    p = np.array([1./6, 2./3, 1./6])\n",
    "    c = np.cumsum(p)\n",
    "    randdoubles = prng.random_sample(new_dim*old_dim)\n",
    "    R = np.searchsorted(c, randdoubles)\n",
    "    R = math.sqrt(3)*(R - 1)\n",
    "    R = np.reshape(R, (new_dim, old_dim))\n",
    "    \n",
    "    M_red = np.dot(R, M)\n",
    "    return M_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recoverL2 algorithm\n",
    "For two words in a document $w_1,w_2$ and their topic assignments $z_1,z_1$ we have the following identities:\n",
    "1. $A[i,k] = p(w_1 = i \\vert z_1 = k)$  \n",
    "2. $Q[i,j] = p(w_1 = i, w_2 = j)$  \n",
    "3. $\\bar{Q}[i,j] = p(w_2 = j \\vert w_1 = i)$  \n",
    "\n",
    "Let $S$ be the set of anchor word indexes. The convex hull formed by the rows of $\\bar{Q}$ indexed by elements of $S$ contains all other rows of $\\bar{Q}$. To see this, note that for anchor words $s_k$, we can show $$\\bar{Q}[s_k,j] = p(w_2 = j \\vert z_1 = k).$$ For all other words $i$, we also have $$\\bar{Q}[i,j] = \\sum_k p(z_1 = k \\vert w_1 = i) p(w_2 = j \\vert z_1 = k).$$ If we denote $p(z_1 = k \\vert w_1 = i)$ as the coefficent $\\alpha_{i,k}$ we can rewrite the above expression more compactly as $\\bar{Q}[i,j] = \\sum_k \\alpha[i,k] \\bar{Q}[s_k,j].$ Since $\\alpha$'s rows consist of non-negative probabilities that sum to one, we have that any row of $\\bar{Q}$ lies in the convex hull of the rows corresponding to the anchor words. The mixing weights of these convex combinations give us $p(z_1 \\vert w_1 = i).$ Using this togeather with $p(w_1 = i)$, we can recover $A$ by applying Bayes' rule: $$A[i,k] = p(w_1 \\vert z_1 = k) = \\frac{p(z_1 \\vert w_1 = i) p(w_1 = i)}{\\sum_{i'} p(z_1 = k | w_1 = i') p(w_1 = i')}.$$ We calculate $p(w_1 = i)$ in terms of $Q$ by noting the following equation $$\\sum_j Q[i,j] = \\sum_j p(w_1 = i, w_2 = j) = p(w_1 = i).$$ With all the terms finally computed, we recover $A$ via the application of Bayes' theorem above.  \n",
    "\n",
    "For each row of the empirical row normalized co-occurance matrix $\\hat{Q}$, `recoverL2` finds the coefficents $p(z_1 \\vert w_1 = i)$ that best reconstruct the row as a convex combination of rows corresponding to anchor words using the exponentiated gradient algorithm `quadSolveExpGrad`. Then by applying Bayes' theorem as before, `recoverL2` can recover $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recoverL2(Q, anchors, eps=10**(-7)):\n",
    "    \"\"\"\n",
    "    Input: Matrix Q, Set of words S, tolerance parameter eps\n",
    "    Output: Matrices A,R\n",
    "    \"\"\"\n",
    "    Qold = np.copy(Q)\n",
    "    V = Q.shape[0]\n",
    "    K = len(anchors)\n",
    "    A = np.matrix(np.zeros((V,K)))\n",
    "    iterations = []\n",
    "    \n",
    "    # Store the normalization constnts P_w = Q*ones\n",
    "    P_w = np.matrix(np.diag(np.dot(Q, np.ones(V))))\n",
    "    for v in xrange(V):\n",
    "        if np.isnan(P_w[v,v]):\n",
    "            P_w[v,v] = 10**(-16)\n",
    "    \n",
    "    #normalize the rows of Q_prime\n",
    "    for v in xrange(V):\n",
    "        Q[v,:] = Q[v,:]/Q[v,:].sum()\n",
    "            \n",
    "    X = Q[anchors, :]\n",
    "    XXT = np.dot(X, X.transpose())\n",
    "\n",
    "    for i in xrange(V):\n",
    "        y = Q[i, :]\n",
    "        alpha, it = recover(y,X,i,anchors,XXT,eps)\n",
    "        A[i, :] = alpha\n",
    "        iterations.append(it)\n",
    "    \n",
    "    #rescale A matrix\n",
    "    #Bayes rule says P(w|z) proportional to P(z|w)P(w)\n",
    "\n",
    "    A = P_w * A\n",
    "\n",
    "    #normalize columns of A. This is the normalization constant P(z)\n",
    "    colsums = A.sum(0)\n",
    "\n",
    "    for k in xrange(K):\n",
    "        A[:, k] = A[:, k]/A[:,k].sum()\n",
    "    \n",
    "    A = np.array(A)\n",
    "#     R = np.dot(np.linalg.pinv(A), np.dot(Qold, np.linalg.pinv(A).T))\n",
    "#     R = np.dot(A.conj().T, np.dot(Qold, A.conj()))\n",
    "\n",
    "    return A, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recover(y,x,i,anchors,XXT,eps):\n",
    "    K = len(anchors)\n",
    "    alpha = np.zeros(K)\n",
    "    gap = None\n",
    "    if i in anchors:\n",
    "        alpha[anchors.index(i)] = 1\n",
    "        it = -1\n",
    "        dist = 0\n",
    "        stepsize = 0\n",
    "\n",
    "    else:\n",
    "        # alpha, it, dist, stepsize, gap = quadSolveExpGrad(y, x, eps, None, XXT)\n",
    "        alpha, it = quadSolveExpGrad(y, x, eps, None, XXT)\n",
    "        if np.isnan(alpha).any():\n",
    "                alpha = np.ones(K)/K\n",
    "\n",
    "    return alpha, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadSolveExpGrad(y, X, eps, alpha=None, XX=None): \n",
    "    \"\"\"Exponentiated gradient method\n",
    "    Input: vector y, matrix X, tolerance parameter eps\n",
    "    non-negative normalized vector alpha close to alpha'\n",
    "    that minimizes the L2 distance between y and X*alpha\n",
    "    finds alpha within eps of the alpha' minimizing the objective function obj(y, Xa) = aXXa - 2*aXY + YY,\n",
    "    where aXXA = dot(dot(alpha, dot(x, x.transpose())), alpha.transpose())\n",
    "    and aXY and YY are defined similarly.\n",
    "    Output: non-negative normalized vector alpha close to alpha' that minimizes the L2 distance between y and X*alpha,\n",
    "    number of iterations it,\n",
    "    final value of the objective function new_obj,\n",
    "    amount to change alpha by stepsize,\n",
    "    closeness to minimal alpha solution gap\"\"\"\n",
    "    \n",
    "    c1 = 10**(-4)\n",
    "    c2 = 0.75\n",
    "    if XX is None:\n",
    "        print 'making XXT'\n",
    "        XX = np.dot(X, X.transpose())\n",
    "\n",
    "    XY = np.dot(X, y)\n",
    "    YY = float(np.dot(y, y))\n",
    "\n",
    "\n",
    "    (K,n) = X.shape\n",
    "#     if alpha is None:\n",
    "#         alpha = np.ones(K)/K\n",
    "    alpha = np.ones(K)/K\n",
    "\n",
    "    old_alpha = np.copy(alpha)\n",
    "    log_alpha = np.log(alpha)\n",
    "    old_log_alpha = np.copy(log_alpha)\n",
    "\n",
    "    it = 1 \n",
    "    aXX = np.dot(alpha, XX)\n",
    "    aXY = float(np.dot(alpha, XY))\n",
    "    aXXa = float(np.dot(aXX, alpha.transpose()))\n",
    "\n",
    "    grad = 2*(aXX-XY)\n",
    "    new_obj = aXXa - 2*aXY + YY\n",
    "\n",
    "    old_grad = np.copy(grad)\n",
    "\n",
    "    stepsize = 1\n",
    "    repeat = False\n",
    "    decreased = False\n",
    "    gap = float('inf')\n",
    "\n",
    "    while 1:\n",
    "        eta = stepsize\n",
    "        old_obj = new_obj\n",
    "        old_alpha = np.copy(alpha)\n",
    "        old_log_alpha = np.copy(log_alpha)\n",
    "\n",
    "        if new_obj == 0 or stepsize == 0:\n",
    "            break\n",
    "\n",
    "#         if it % 1000 == 0:\n",
    "#             print \"\\titer\", it, new_obj, gap, stepsize\n",
    "        it += 1\n",
    "        #update\n",
    "        log_alpha -= eta*grad\n",
    "        #normalize\n",
    "        log_alpha -= logsum_exp(log_alpha)\n",
    "        #compute new objective\n",
    "        alpha = np.exp(log_alpha)\n",
    "\n",
    "        aXX = np.dot(alpha, XX)\n",
    "        aXY = float(np.dot(alpha, XY))\n",
    "        aXXa = float(np.dot(aXX, alpha.transpose()))\n",
    "\n",
    "        old_obj = new_obj\n",
    "        new_obj = aXXa - 2*aXY + YY\n",
    "#         old_obj, new_obj = new_obj, aXXa - 2 * aXY + YY\n",
    "        if not new_obj <= old_obj + c1*stepsize*np.dot(grad, alpha - old_alpha): #sufficient decrease\n",
    "            stepsize /= 2.0 #reduce stepsize\n",
    "            alpha = old_alpha \n",
    "            log_alpha = old_log_alpha\n",
    "            new_obj = old_obj\n",
    "            repeat = True\n",
    "            decreased = True\n",
    "            continue\n",
    "\n",
    "        #compute the new gradient\n",
    "        old_grad = np.copy(grad)\n",
    "        grad = 2*(aXX-XY)\n",
    "#         old_obj, new_obj = new_obj, aXXa - 2 * aXY + YY\n",
    "        \n",
    "        if (not np.dot(grad, alpha - old_alpha) >= c2*np.dot(old_grad, alpha-old_alpha)) and (not decreased): #curvature\n",
    "            stepsize *= 2.0 #increase stepsize\n",
    "            alpha = old_alpha\n",
    "            log_alpha = old_log_alpha\n",
    "            grad = old_grad\n",
    "            new_obj = old_obj\n",
    "            repeat = True\n",
    "            continue\n",
    "\n",
    "        decreased = False\n",
    "\n",
    "        lam = np.copy(grad)\n",
    "        lam -= lam.min()\n",
    "        \n",
    "        gap = np.dot(alpha, lam)\n",
    "        convergence = gap\n",
    "        if (convergence < eps):\n",
    "            break\n",
    "\n",
    "    return alpha, it #, it, new_obj, stepsize, gap\n",
    "\n",
    "def logsum_exp(y):\n",
    "    m = y.max()\n",
    "    return m + np.log((np.exp(y - m)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import wget\n",
    "import gzip\n",
    "#import scipy\n",
    "# from sklearn import random_projection\n",
    "import time\n",
    "import math\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# matplotlib.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have any datasets for topic modeling, here are a couple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nips.txt\"\n",
    "docwordUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nips.txt.gz\"\n",
    "wget.download(vocabUrl)\n",
    "wget.download(docwordUrl)\n",
    "\n",
    "# NY Times articles dataset (~300,000 documents, ~100,00 vocabulary, ~100,000,000 words)\n",
    "# vocabUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nytimes.txt\"\n",
    "# docwordUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nytimes.txt.gz\"\n",
    "# wget.download(vocabUrl)\n",
    "# wget.download(docwordUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the dataset into a sparse array below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening file\n",
      "number of docs = 300000, number of words = 102660, number nonzero = 69679427\n",
      "constructing word-document matrix\n",
      "0 0.00190401077271 1 413 1\n",
      "100000 0.503263950348 435 15627 1\n",
      "200000 0.410134077072 875 40446 1\n",
      "300000 0.420979976654 1343 48964 1\n",
      "400000 0.378053188324 1818 35083 1\n",
      "500000 0.371774196625 2277 26610 1\n",
      "600000 0.373458862305 2736 30051 2\n",
      "700000 0.407560110092 3201 27006 2\n",
      "800000 0.419481992722 3672 22968 1\n",
      "900000 0.3887591362 4139 33955 1\n",
      "1000000 0.40186214447 4582 30239 1\n",
      "1100000 0.385185003281 5013 28051 4\n",
      "1200000 0.408826112747 5471 22924 2\n",
      "1300000 0.390644788742 5955 19865 1\n",
      "1400000 0.376908063889 6432 16376 1\n",
      "1500000 0.395451068878 6891 20298 1\n",
      "1600000 0.387594938278 7318 39838 2\n",
      "1700000 0.369194984436 7802 44101 9\n",
      "1800000 0.399671077728 8262 444 1\n",
      "1900000 0.456289052963 8716 30246 1\n",
      "2000000 0.466792106628 9178 1907 2\n",
      "2100000 0.405920982361 9622 22554 2\n",
      "2200000 0.4381711483 10069 37424 1\n",
      "2300000 0.411858081818 10527 13603 1\n",
      "2400000 0.40091919899 10962 5273 4\n",
      "2500000 0.403549909592 11438 19769 1\n",
      "2600000 0.405367136002 11882 24877 1\n",
      "2700000 0.377372026443 12318 24366 1\n",
      "2800000 0.414604902267 12808 32091 2\n",
      "2900000 0.399574041367 13286 5526 1\n",
      "3000000 0.404263973236 13741 34766 1\n",
      "3100000 0.408223867416 14204 7633 1\n",
      "3200000 0.383745193481 14635 25952 1\n",
      "3300000 0.391577959061 15096 43808 4\n",
      "3400000 0.404804944992 15572 17601 1\n",
      "3500000 0.386554002762 16026 45568 1\n",
      "3600000 0.352821111679 16509 23237 1\n",
      "3700000 0.386442899704 16960 26087 1\n",
      "3800000 0.386651992798 17418 24307 1\n",
      "3900000 0.390892982483 17864 1700 1\n",
      "4000000 0.372473955154 18330 43981 1\n",
      "4100000 0.381864070892 18754 30055 2\n",
      "4200000 0.377218008041 19164 16453 1\n",
      "4300000 0.411111116409 19626 8178 1\n",
      "4400000 0.39243388176 20092 38341 1\n",
      "4500000 0.378609895706 20544 37851 1\n",
      "4600000 0.398716926575 21015 101230 1\n",
      "4700000 0.395417928696 21464 9942 1\n",
      "4800000 0.402259111404 21909 12261 1\n",
      "4900000 0.401494979858 22370 35364 1\n",
      "5000000 0.414067983627 22815 29150 1\n",
      "5100000 0.420001029968 23265 7770 1\n",
      "5200000 0.446609973907 23712 9571 2\n",
      "5300000 0.420731067657 24172 19882 1\n",
      "5400000 0.382136821747 24603 24877 5\n",
      "5500000 0.385685920715 25058 6633 2\n",
      "5600000 0.37980389595 25516 29176 1\n",
      "5700000 0.39271903038 25978 35546 1\n",
      "5800000 0.44570016861 26408 41567 1\n",
      "5900000 0.406001091003 26872 16673 1\n",
      "6000000 0.407749891281 27343 20073 2\n",
      "6100000 0.378976106644 27789 38668 1\n",
      "6200000 0.381170034409 28258 18338 1\n",
      "6300000 0.392524003983 28720 14395 2\n",
      "6400000 0.408720016479 29141 16920 1\n",
      "6500000 0.379575967789 29591 12924 1\n",
      "6600000 0.382764101028 30050 38099 1\n",
      "6700000 0.375430822372 30491 27032 1\n",
      "6800000 0.387049913406 30920 5103 1\n",
      "6900000 0.380164146423 31380 33807 1\n",
      "7000000 0.383322000504 31818 19502 1\n",
      "7100000 0.360556125641 32287 14493 1\n",
      "7200000 0.374685049057 32726 6476 1\n",
      "7300000 0.398076057434 33169 9790 1\n",
      "7400000 0.394944906235 33632 38865 1\n",
      "7500000 0.412767887115 34098 38047 1\n",
      "7600000 0.411329030991 34528 2115 1\n",
      "7700000 0.470789909363 34917 11862 1\n",
      "7800000 0.38360619545 35340 1979 2\n",
      "7900000 0.38861489296 35771 23707 1\n",
      "8000000 0.390772104263 36224 35503 2\n",
      "8100000 0.502281904221 36665 22391 1\n",
      "8200000 0.597734212875 37127 6977 6\n",
      "8300000 0.54946398735 37596 2829 1\n",
      "8400000 0.558947086334 38036 12525 1\n",
      "8500000 0.649721860886 38484 18447 1\n",
      "8600000 0.593005895615 38947 29929 2\n",
      "8700000 0.61639881134 39411 677 1\n",
      "8800000 0.390761852264 39842 25391 2\n",
      "8900000 0.419394016266 40284 34113 1\n",
      "9000000 0.391164064407 40743 33164 1\n",
      "9100000 0.393019199371 41188 10396 1\n",
      "9200000 0.399678945541 41626 317 1\n",
      "9300000 0.41205406189 42064 18806 1\n",
      "9400000 0.395318031311 42512 19348 1\n",
      "9500000 0.388251066208 42982 8302 1\n",
      "9600000 0.415455102921 43420 36780 1\n",
      "9700000 0.435943126678 43866 24958 1\n",
      "9800000 0.469455003738 44333 80497 1\n",
      "9900000 0.439846038818 44776 29544 2\n",
      "10000000 0.483114957809 45202 30797 1\n",
      "10100000 0.428833007812 45626 29653 1\n",
      "10200000 0.401105880737 46083 29167 1\n",
      "10300000 0.381557941437 46539 25156 1\n",
      "10400000 0.392175197601 46969 41050 4\n",
      "10500000 0.398866176605 47413 22713 1\n",
      "10600000 0.426625013351 47856 7658 1\n",
      "10700000 0.484161138535 48297 31153 2\n",
      "10800000 0.448659896851 48758 26819 1\n",
      "10900000 0.395653009415 49190 50757 1\n",
      "11000000 0.427720069885 49629 26703 1\n",
      "11100000 0.412993907928 50065 7184 1\n",
      "11200000 0.552567005157 50492 35139 1\n",
      "11300000 0.653245925903 50924 19572 1\n",
      "11400000 0.664703845978 51345 87700 1\n",
      "11500000 0.643520832062 51769 27185 2\n",
      "11600000 0.613464832306 52212 6020 1\n",
      "11700000 0.390486001968 52624 458 2\n",
      "11800000 0.398175954819 53059 40281 1\n",
      "11900000 0.416127920151 53479 22871 1\n",
      "12000000 0.399525880814 53897 17004 2\n",
      "12100000 0.463064908981 54339 37851 1\n",
      "12200000 0.434617042542 54790 43114 1\n",
      "12300000 0.389945983887 55262 42837 1\n",
      "12400000 0.43240404129 55678 38637 1\n",
      "12500000 0.404181957245 56130 743 1\n",
      "12600000 0.396552801132 56566 31259 1\n",
      "12700000 0.429852962494 56975 6946 1\n",
      "12800000 0.422571182251 57408 33550 1\n",
      "12900000 0.623821020126 57834 98678 2\n",
      "13000000 0.470355033875 58265 33472 1\n",
      "13100000 0.515406131744 58704 1058 1\n",
      "13200000 0.558115959167 59119 32320 1\n",
      "13300000 0.522469997406 59574 35542 2\n",
      "13400000 0.492100000381 60014 20073 1\n",
      "13500000 0.602972984314 60430 28804 1\n",
      "13600000 0.629576921463 60872 9485 1\n",
      "13700000 0.54249215126 61311 12044 1\n",
      "13800000 0.434674978256 61729 53849 4\n",
      "13900000 0.400952100754 62182 17185 1\n",
      "14000000 0.376919031143 62574 18072 1\n",
      "14100000 0.368422031403 63027 10317 1\n",
      "14200000 0.403224945068 63457 70587 8\n",
      "14300000 0.38392496109 63883 7433 1\n",
      "14400000 0.405157089233 64328 99200 1\n",
      "14500000 0.409352064133 64752 37526 1\n",
      "14600000 0.391320943832 65152 8336 1\n",
      "14700000 0.381680011749 65596 13263 1\n",
      "14800000 0.379031181335 66023 30087 4\n",
      "14900000 0.482746124268 66466 13701 1\n",
      "15000000 0.42452287674 66900 93985 7\n",
      "15100000 0.434994935989 67315 15796 1\n",
      "15200000 0.572653770447 67746 101090 1\n",
      "15300000 0.435988903046 68182 16673 1\n",
      "15400000 0.370368003845 68618 378 1\n",
      "15500000 0.395349025726 69048 20783 2\n",
      "15600000 0.406992912292 69482 12748 1\n",
      "15700000 0.409555196762 69891 18449 1\n",
      "15800000 0.414994001389 70315 16668 1\n",
      "15900000 0.389955043793 70764 11580 2\n",
      "16000000 0.425911903381 71198 15517 1\n",
      "16100000 0.431579828262 71630 22103 1\n",
      "16200000 0.550317049026 72062 59774 2\n",
      "16300000 0.406570911407 72479 29408 1\n",
      "16400000 0.400006055832 72926 32803 1\n",
      "16500000 0.426864862442 73365 22502 1\n",
      "16600000 0.389312028885 73780 26596 1\n",
      "16700000 0.355988025665 74214 8652 1\n",
      "16800000 0.387480020523 74642 15694 1\n",
      "16900000 0.403448104858 75069 26444 1\n",
      "17000000 0.379764080048 75495 157 1\n",
      "17100000 0.399389028549 75944 40060 1\n",
      "17200000 0.360882043839 76363 17654 2\n",
      "17300000 0.396122932434 76821 58703 2\n",
      "17400000 0.355675935745 77266 29073 1\n",
      "17500000 0.367444038391 77696 8146 1\n",
      "17600000 0.382275104523 78136 38018 1\n",
      "17700000 0.379757881165 78572 1173 1\n",
      "17800000 0.372172832489 79021 68398 1\n",
      "17900000 0.369949817657 79425 14282 1\n",
      "18000000 0.427332878113 79853 25961 3\n",
      "18100000 0.397439002991 80307 49323 1\n",
      "18200000 0.402181863785 80749 10622 1\n",
      "18300000 0.408034086227 81202 38752 1\n",
      "18400000 0.448082923889 81636 10510 1\n",
      "18500000 0.391067028046 82066 604 1\n",
      "18600000 0.416344881058 82506 37279 1\n",
      "18700000 0.3735871315 82947 42065 1\n",
      "18800000 0.403198003769 83391 8107 1\n",
      "18900000 0.391125917435 83808 758 1\n",
      "19000000 0.382484912872 84242 9329 1\n",
      "19100000 0.366802930832 84672 17480 1\n",
      "19200000 0.377570867538 85064 23247 1\n",
      "19300000 0.40848493576 85500 33164 1\n",
      "19400000 0.407348871231 85918 29562 4\n",
      "19500000 0.383283853531 86354 35370 1\n",
      "19600000 0.410691022873 86765 7374 1\n",
      "19700000 0.396389961243 87191 34853 1\n",
      "19800000 0.399009943008 87633 34522 2\n",
      "19900000 0.379870176315 88050 17933 1\n",
      "20000000 0.373200178146 88459 101972 1\n",
      "20100000 0.384163141251 88875 31439 1\n",
      "20200000 0.372355937958 89308 30442 1\n",
      "20300000 0.372204065323 89734 8224 1\n",
      "20400000 0.396071910858 90166 35238 1\n",
      "20500000 0.369292020798 90569 15401 2\n",
      "20600000 0.397277116776 91015 37856 1\n",
      "20700000 0.398376941681 91448 33403 1\n",
      "20800000 0.414650917053 91866 61395 1\n",
      "20900000 0.357455968857 92301 17452 1\n",
      "21000000 0.370692014694 92690 14404 1\n",
      "21100000 0.38356089592 93125 23241 1\n",
      "21200000 0.387934923172 93554 9359 1\n",
      "21300000 0.387848138809 93988 33093 3\n",
      "21400000 0.383059978485 94445 14007 1\n",
      "21500000 0.376393079758 94887 43826 2\n",
      "21600000 0.367913961411 95306 98731 1\n",
      "21700000 0.380281925201 95754 17722 1\n",
      "21800000 0.401842832565 96189 38739 1\n",
      "21900000 0.387986898422 96631 28467 1\n",
      "22000000 0.366189002991 97077 8112 1\n",
      "22100000 0.387752056122 97518 62905 1\n",
      "22200000 0.364789009094 97947 35560 2\n",
      "22300000 0.358018159866 98395 40887 2\n",
      "22400000 0.377631902695 98831 30890 1\n",
      "22500000 0.371526956558 99241 34725 1\n",
      "22600000 0.396420955658 99656 12559 1\n",
      "22700000 0.380505800247 100098 101048 3\n",
      "22800000 0.366557836533 100550 27992 2\n",
      "22900000 0.389050006866 100979 26699 1\n",
      "23000000 0.39946603775 101408 30678 1\n",
      "23100000 0.387171030045 101839 4868 1\n",
      "23200000 0.370178937912 102282 30593 1\n",
      "23300000 0.377079963684 102725 37033 2\n",
      "23400000 0.379348039627 103172 6630 1\n",
      "23500000 0.375881910324 103629 8226 1\n",
      "23600000 0.368575811386 104059 11454 1\n",
      "23700000 0.351316213608 104469 65650 1\n",
      "23800000 0.396744012833 104901 56860 1\n",
      "23900000 0.375719070435 105314 38433 1\n",
      "24000000 0.374863862991 105751 6111 1\n",
      "24100000 0.369286060333 106180 29073 1\n",
      "24200000 0.383643865585 106606 20073 3\n",
      "24300000 0.397305965424 107060 44053 2\n",
      "24400000 0.376100063324 107499 16638 1\n",
      "24500000 0.363826036453 107946 32480 1\n",
      "24600000 0.410572052002 108376 6069 2\n",
      "24700000 0.363734960556 108782 12094 1\n",
      "24800000 0.3603951931 109181 15268 1\n",
      "24900000 0.375324010849 109602 44138 1\n",
      "25000000 0.396994113922 110042 43708 1\n",
      "25100000 0.382894039154 110461 29184 3\n",
      "25200000 0.377123117447 110921 42762 1\n",
      "25300000 0.372385978699 111337 21924 1\n",
      "25400000 0.387305021286 111760 94578 2\n",
      "25500000 0.37627196312 112191 43489 2\n",
      "25600000 0.397883176804 112557 16124 1\n",
      "25700000 0.371416091919 113014 13966 1\n",
      "25800000 0.376181840897 113461 11554 1\n",
      "25900000 0.386080026627 113880 63560 1\n",
      "26000000 0.378665924072 114310 39270 1\n",
      "26100000 0.374661207199 114725 596 1\n",
      "26200000 0.380033016205 115124 35868 1\n",
      "26300000 0.361630916595 115562 64155 17\n",
      "26400000 0.383407831192 116002 3888 1\n",
      "26500000 0.372975826263 116423 63088 2\n",
      "26600000 0.378364086151 116867 1075 1\n",
      "26700000 0.396398067474 117313 14656 1\n",
      "26800000 0.374184846878 117744 45054 1\n",
      "26900000 0.370638132095 118168 21803 2\n",
      "27000000 0.373817205429 118601 44333 3\n",
      "27100000 0.382030010223 119050 29142 1\n",
      "27200000 0.371192932129 119509 31508 1\n",
      "27300000 0.382463932037 119922 41161 1\n",
      "27400000 0.371189832687 120339 38126 2\n",
      "27500000 0.39543390274 120771 5560 1\n",
      "27600000 0.36088180542 121183 25148 2\n",
      "27700000 0.410577058792 121619 18037 1\n",
      "27800000 0.40158200264 122026 32266 1\n",
      "27900000 0.355082035065 122474 33616 1\n",
      "28000000 0.379724025726 122901 39490 1\n",
      "28100000 0.373748064041 123353 29562 1\n",
      "28200000 0.384097099304 123802 24461 1\n",
      "28300000 0.385922908783 124229 12430 2\n",
      "28400000 0.385792970657 124668 8670 1\n",
      "28500000 0.373183012009 125102 56676 1\n",
      "28600000 0.376636981964 125540 4906 1\n",
      "28700000 0.38631105423 125964 42354 1\n",
      "28800000 0.392466068268 126401 33550 1\n",
      "28900000 0.378993988037 126814 16043 1\n",
      "29000000 0.385815858841 127260 19517 1\n",
      "29100000 0.407730817795 127679 33923 2\n",
      "29200000 0.393194913864 128135 15716 2\n",
      "29300000 0.416043043137 128572 24252 1\n",
      "29400000 0.40861582756 128993 51840 3\n",
      "29500000 0.375117063522 129420 101934 2\n",
      "29600000 0.390542984009 129875 42891 1\n",
      "29700000 0.394471168518 130328 6338 1\n",
      "29800000 0.365067958832 130840 21541 1\n",
      "29900000 0.376448154449 131276 25526 1\n",
      "30000000 0.378138065338 131688 57730 3\n",
      "30100000 0.393275976181 132119 15432 1\n",
      "30200000 0.365318775177 132562 13513 2\n",
      "30300000 0.357840061188 132974 33640 1\n",
      "30400000 0.384772062302 133410 40963 1\n",
      "30500000 0.386358976364 133847 29232 1\n",
      "30600000 0.397887945175 134261 27022 1\n",
      "30700000 0.382924079895 134662 23947 2\n",
      "30800000 0.36229300499 135077 28033 1\n",
      "30900000 0.372066020966 135502 13862 1\n",
      "31000000 0.353055953979 135959 29671 1\n",
      "31100000 0.407764911652 136397 15385 1\n",
      "31200000 0.380100011826 136852 19168 1\n",
      "31300000 0.372048854828 137276 43474 1\n",
      "31400000 0.377815008163 137721 2368 1\n",
      "31500000 0.359855175018 138154 34837 1\n",
      "31600000 0.370112895966 138580 38075 1\n",
      "31700000 0.383727073669 139022 32557 1\n",
      "31800000 0.387398958206 139442 22635 1\n",
      "31900000 0.406569004059 139892 37948 1\n",
      "32000000 0.369883060455 140320 27840 1\n",
      "32100000 0.431061029434 140748 38106 1\n",
      "32200000 0.364508152008 141179 37194 1\n",
      "32300000 0.350764036179 141612 400 1\n",
      "32400000 0.357378005981 142026 17004 4\n",
      "32500000 0.384519100189 142485 42643 1\n",
      "32600000 0.38237118721 142914 3044 2\n",
      "32700000 0.37121295929 143348 26995 1\n",
      "32800000 0.402797937393 143787 30538 2\n",
      "32900000 0.3695499897 144201 34376 1\n",
      "33000000 0.401747941971 144621 36870 1\n",
      "33100000 0.407429933548 145042 14261 1\n",
      "33200000 0.374308824539 145492 35032 1\n",
      "33300000 0.389146089554 145965 23667 1\n",
      "33400000 0.398488044739 146383 39855 1\n",
      "33500000 0.376806974411 146843 1177 1\n",
      "33600000 0.383987188339 147281 4861 8\n",
      "33700000 0.368109226227 147724 39146 1\n",
      "33800000 0.364235162735 148165 18445 1\n",
      "33900000 0.384086847305 148581 15553 1\n",
      "34000000 0.375873088837 149008 10569 1\n",
      "34100000 0.406891107559 149455 43187 1\n",
      "34200000 0.370411872864 149854 18200 6\n",
      "34300000 0.386718988419 150307 15269 1\n",
      "34400000 0.378431081772 150776 13766 1\n",
      "34500000 0.388700008392 151227 1706 1\n",
      "34600000 0.378055095673 151642 26701 2\n",
      "34700000 0.378654003143 152105 12447 1\n",
      "34800000 0.364398002625 152554 27458 1\n",
      "34900000 0.3804500103 153001 10983 1\n",
      "35000000 0.352373123169 153448 15794 1\n",
      "35100000 0.387781858444 153813 35570 5\n",
      "35200000 0.370800018311 154236 34771 1\n",
      "35300000 0.356244802475 154690 4729 1\n",
      "35400000 0.376112937927 155092 34887 1\n",
      "35500000 0.376272916794 155521 17572 2\n",
      "35600000 0.402463197708 155962 21560 1\n",
      "35700000 0.369699954987 156378 30112 1\n",
      "35800000 0.363369941711 156819 91283 1\n",
      "35900000 0.389720916748 157262 19022 1\n",
      "36000000 0.374465942383 157710 7464 1\n",
      "36100000 0.380410194397 158132 1224 1\n",
      "36200000 0.370640993118 158562 9634 1\n",
      "36300000 0.359055995941 158989 44186 1\n",
      "36400000 0.357152938843 159450 40807 1\n",
      "36500000 0.391805887222 159888 67441 8\n",
      "36600000 0.379220962524 160320 23216 1\n",
      "36700000 0.387424945831 160743 38237 1\n",
      "36800000 0.366162061691 161173 510 1\n",
      "36900000 0.369379997253 161605 7418 1\n",
      "37000000 0.364943027496 162060 36050 1\n",
      "37100000 0.397178173065 162485 20774 1\n",
      "37200000 0.384984970093 162940 40235 2\n",
      "37300000 0.386955976486 163393 31865 1\n",
      "37400000 0.394432783127 163826 18449 5\n",
      "37500000 0.377316951752 164267 5528 1\n",
      "37600000 0.408934831619 164693 29562 1\n",
      "37700000 0.354923963547 165139 27600 1\n",
      "37800000 0.386292934418 165575 6974 1\n",
      "37900000 0.381325006485 166021 48952 2\n",
      "38000000 0.396978139877 166461 6888 1\n",
      "38100000 0.381084918976 166884 50524 1\n",
      "38200000 0.415030956268 167290 13163 1\n",
      "38300000 0.39811205864 167731 8323 2\n",
      "38400000 0.378969907761 168164 23424 1\n",
      "38500000 0.380000114441 168590 26866 1\n",
      "38600000 0.399115085602 169027 37982 1\n",
      "38700000 0.383852005005 169472 13203 1\n",
      "38800000 0.381701946259 169923 12821 4\n",
      "38900000 0.387901067734 170369 89156 1\n",
      "39000000 0.389496803284 170782 28179 1\n",
      "39100000 0.381922960281 171239 33069 1\n",
      "39200000 0.370846986771 171702 40228 1\n",
      "39300000 0.37779712677 172128 38340 1\n",
      "39400000 0.368097782135 172549 28154 1\n",
      "39500000 0.371243953705 172968 11457 1\n",
      "39600000 0.381317853928 173433 65208 1\n",
      "39700000 0.368113994598 173913 18828 1\n",
      "39800000 0.378071069717 174347 21308 2\n",
      "39900000 0.39147901535 174799 55991 8\n",
      "40000000 0.383489131927 175235 20705 1\n",
      "40100000 0.372420072556 175697 2153 1\n",
      "40200000 0.380867958069 176166 41351 1\n",
      "40300000 0.375909090042 176619 18002 1\n",
      "40400000 0.38000202179 177058 3753 1\n",
      "40500000 0.387625932693 177469 14395 1\n",
      "40600000 0.38251709938 177917 101972 1\n",
      "40700000 0.38275885582 178380 37308 1\n",
      "40800000 0.393537998199 178818 33674 1\n",
      "40900000 0.400352954865 179276 24385 7\n",
      "41000000 0.397746086121 179707 91803 5\n",
      "41100000 0.40758395195 180128 8305 1\n",
      "41200000 0.368914842606 180579 15767 2\n",
      "41300000 0.372932910919 181054 4527 1\n",
      "41400000 0.375535011292 181509 43575 1\n",
      "41500000 0.398996114731 181930 18447 1\n",
      "41600000 0.36891913414 182392 13524 1\n",
      "41700000 0.377980947495 182828 6083 1\n",
      "41800000 0.365365982056 183294 28580 1\n",
      "41900000 0.372213840485 183741 42982 1\n",
      "42000000 0.392991065979 184180 23974 1\n",
      "42100000 0.367941856384 184610 35965 1\n",
      "42200000 0.421017169952 185079 44037 1\n",
      "42300000 0.374319791794 185514 21654 1\n",
      "42400000 0.389790058136 185940 13130 1\n",
      "42500000 0.384036064148 186361 33167 1\n",
      "42600000 0.399602174759 186796 6665 1\n",
      "42700000 0.381796121597 187253 49540 1\n",
      "42800000 0.370679140091 187734 9602 3\n",
      "42900000 0.377444982529 188164 806 1\n",
      "43000000 0.365287065506 188581 82236 5\n",
      "43100000 0.370614051819 189027 43232 1\n",
      "43200000 0.388137102127 189470 27461 1\n",
      "43300000 0.380868196487 189923 30096 1\n",
      "43400000 0.392253875732 190314 13508 1\n",
      "43500000 0.379103899002 190770 9 1\n",
      "43600000 0.378598928452 191179 32802 1\n",
      "43700000 0.374402999878 191578 16253 2\n",
      "43800000 0.363656997681 191991 376 1\n",
      "43900000 0.377431154251 192413 33676 1\n",
      "44000000 0.382308959961 192785 43579 1\n",
      "44100000 0.370270013809 193187 14628 1\n",
      "44200000 0.36252117157 193591 32999 1\n",
      "44300000 0.387184858322 194025 42963 1\n",
      "44400000 0.385849952698 194457 39607 9\n",
      "44500000 0.365421056747 194843 1267 1\n",
      "44600000 0.381690979004 195275 99105 1\n",
      "44700000 0.39271903038 195671 13728 1\n",
      "44800000 0.372232913971 196076 30008 1\n",
      "44900000 0.372102975845 196518 39144 1\n",
      "45000000 0.403074026108 196947 29438 1\n",
      "45100000 0.421092987061 197375 9682 4\n",
      "45200000 0.391402006149 197810 14112 1\n",
      "45300000 0.416428804398 198198 35454 1\n",
      "45400000 0.422304868698 198619 6651 1\n",
      "45500000 0.479747056961 199070 32150 1\n",
      "45600000 0.420305013657 199504 22537 1\n",
      "45700000 0.386006116867 199938 4696 1\n",
      "45800000 0.38239812851 200339 91281 2\n",
      "45900000 0.378210067749 200730 32803 1\n",
      "46000000 0.41841506958 201150 44192 1\n",
      "46100000 0.443103075027 201545 17119 1\n",
      "46200000 0.396270036697 201965 14626 3\n",
      "46300000 0.457740068436 202385 39129 1\n",
      "46400000 0.411330938339 202806 7169 1\n",
      "46500000 0.44766497612 203207 42891 1\n",
      "46600000 0.392490148544 203611 84344 10\n",
      "46700000 0.392535924911 204029 14978 1\n",
      "46800000 0.473846912384 204435 3686 2\n",
      "46900000 0.58117389679 204856 43020 1\n",
      "47000000 0.604158163071 205276 81992 4\n",
      "47100000 0.599294900894 205659 17192 1\n",
      "47200000 0.569514036179 206087 101958 1\n",
      "47300000 0.472136974335 206527 2340 1\n",
      "47400000 0.426846027374 206939 1325 1\n",
      "47500000 0.516644954681 207375 10900 1\n",
      "47600000 0.561068058014 207763 36214 1\n",
      "47700000 0.576201915741 208187 33550 1\n",
      "47800000 0.425397872925 208628 99726 2\n",
      "47900000 0.628375053406 209042 1700 1\n",
      "48000000 0.517100095749 209463 6889 1\n",
      "48100000 0.48842382431 209853 298 1\n",
      "48200000 0.542288064957 210275 33919 1\n",
      "48300000 0.52493596077 210688 43985 1\n",
      "48400000 0.402356147766 211106 6463 4\n",
      "48500000 0.564275026321 211535 11661 1\n",
      "48600000 0.54231595993 211959 25896 1\n",
      "48700000 0.504606962204 212327 22635 1\n",
      "48800000 0.544220924377 212725 35083 1\n",
      "48900000 0.407529115677 213110 8641 2\n",
      "49000000 0.434752941132 213538 8641 1\n",
      "49100000 0.394309997559 213955 18449 1\n",
      "49200000 0.380666971207 214405 13669 1\n",
      "49300000 0.388516187668 214778 22161 1\n",
      "49400000 0.38706612587 215187 24991 2\n",
      "49500000 0.483138084412 215605 3359 1\n",
      "49600000 0.4135658741 216047 39987 1\n",
      "49700000 0.427881002426 216424 6985 4\n",
      "49800000 0.38011598587 216796 6094 1\n",
      "49900000 0.399776935577 217210 30502 1\n",
      "50000000 0.494476079941 217650 22260 2\n",
      "50100000 0.481374979019 218057 12961 1\n",
      "50200000 0.431564092636 218477 18353 1\n",
      "50300000 0.39878320694 218881 8178 1\n",
      "50400000 0.392602920532 219274 1924 1\n",
      "50500000 0.398571014404 219695 34784 1\n",
      "50600000 0.414610862732 220098 6157 1\n",
      "50700000 0.379230976105 220527 98109 2\n",
      "50800000 0.361013174057 220918 25130 1\n",
      "50900000 0.392835140228 221325 40144 1\n",
      "51000000 0.395524978638 221746 3687 1\n",
      "51100000 0.408461093903 222158 14355 2\n",
      "51200000 0.580495119095 222602 10844 1\n",
      "51300000 0.401551008224 223038 36659 1\n",
      "51400000 0.402511119843 223424 31175 1\n",
      "51500000 0.404097080231 223815 12586 1\n",
      "51600000 0.399318933487 224226 27961 1\n",
      "51700000 0.374083995819 224686 42039 1\n",
      "51800000 0.388175010681 225053 8178 1\n",
      "51900000 0.380542993546 225452 39133 1\n",
      "52000000 0.404783010483 225821 13481 1\n",
      "52100000 0.401379108429 226239 11411 1\n",
      "52200000 0.375895023346 226610 29184 4\n",
      "52300000 0.399087905884 227009 28456 1\n",
      "52400000 0.497533082962 227405 55783 1\n",
      "52500000 0.486851930618 227867 14394 1\n",
      "52600000 0.450284004211 228271 22528 1\n",
      "52700000 0.432907819748 228699 98714 1\n",
      "52800000 0.418745994568 229163 39430 2\n",
      "52900000 0.393867015839 229593 5752 1\n",
      "53000000 0.450161933899 230026 20553 1\n",
      "53100000 0.521542072296 230415 18566 1\n",
      "53200000 0.456562995911 230864 6239 1\n",
      "53300000 0.411807060242 231283 50256 3\n",
      "53400000 0.402278900146 231727 34067 2\n",
      "53500000 0.399863958359 232142 40688 1\n",
      "53600000 0.385852813721 232537 24746 1\n",
      "53700000 0.536733150482 232974 25137 1\n",
      "53800000 0.444690942764 233407 41851 2\n",
      "53900000 0.408118963242 233848 6237 1\n",
      "54000000 0.378798961639 234247 53265 1\n",
      "54100000 0.443114995956 234648 25896 1\n",
      "54200000 0.568173885345 235076 28051 1\n",
      "54300000 0.512334108353 235484 35765 1\n",
      "54400000 0.677831888199 235881 7757 1\n",
      "54500000 0.550253868103 236279 22434 1\n",
      "54600000 0.403787136078 236667 10687 1\n",
      "54700000 0.417205095291 237085 36645 1\n",
      "54800000 0.426687002182 237502 39913 1\n",
      "54900000 0.389916181564 237933 83765 4\n",
      "55000000 0.408823013306 238367 25994 1\n",
      "55100000 0.415351867676 238752 41709 1\n",
      "55200000 0.396449804306 239154 36944 1\n",
      "55300000 0.394955158234 239591 43981 1\n",
      "55400000 0.377690076828 240030 7213 1\n",
      "55500000 0.432496070862 240476 43293 1\n",
      "55600000 0.680577993393 240913 204 2\n",
      "55700000 0.448902130127 241310 28025 2\n",
      "55800000 0.57518696785 241757 35310 1\n",
      "55900000 0.508610010147 242161 31111 1\n",
      "56000000 0.664379119873 242611 33503 1\n",
      "56100000 0.464338064194 243046 27532 1\n",
      "56200000 0.428736925125 243442 21560 2\n",
      "56300000 0.428020000458 243868 19770 1\n",
      "56400000 0.405866146088 244298 14282 1\n",
      "56500000 0.426854133606 244720 9004 2\n",
      "56600000 0.42346906662 245152 67626 3\n",
      "56700000 0.424578905106 245589 40176 1\n",
      "56800000 0.548364162445 245969 29764 1\n",
      "56900000 0.541657924652 246387 453 1\n",
      "57000000 0.469820022583 246808 28150 1\n",
      "57100000 0.641538143158 247215 43241 1\n",
      "57200000 0.519043922424 247647 55483 3\n",
      "57300000 0.464869976044 248073 82152 1\n",
      "57400000 0.455191850662 248486 76426 1\n",
      "57500000 0.412042140961 248914 2900 1\n",
      "57600000 0.428467035294 249331 18566 2\n",
      "57700000 0.420464992523 249754 20780 2\n",
      "57800000 0.437931060791 250191 19796 1\n",
      "57900000 0.390911817551 250621 46159 2\n",
      "58000000 0.401669025421 251035 22203 1\n",
      "58100000 0.410972118378 251476 9827 1\n",
      "58200000 0.406713008881 251914 37945 1\n",
      "58300000 0.449402809143 252370 43220 1\n",
      "58400000 0.407330989838 252809 90191 1\n",
      "58500000 0.391638040543 253212 68713 1\n",
      "58600000 0.396852016449 253624 29663 1\n",
      "58700000 0.374733924866 254065 11698 1\n",
      "58800000 0.376648902893 254506 25833 3\n",
      "58900000 0.403294086456 254935 37470 1\n",
      "59000000 0.386383056641 255330 10744 1\n",
      "59100000 0.383081912994 255777 16852 3\n",
      "59200000 0.395091056824 256210 9578 1\n",
      "59300000 0.408476114273 256657 20217 1\n",
      "59400000 0.384328126907 257090 15151 1\n",
      "59500000 0.383643865585 257485 15674 1\n",
      "59600000 0.382053852081 257888 28475 1\n",
      "59700000 0.426945924759 258298 26761 2\n",
      "59800000 0.392810106277 258711 43013 2\n",
      "59900000 0.393615007401 259168 31347 1\n",
      "60000000 0.385488033295 259611 33805 1\n",
      "60100000 0.390285015106 259993 30087 1\n",
      "60200000 0.397920846939 260412 49132 1\n",
      "60300000 0.396046876907 260832 29408 1\n",
      "60400000 0.389078140259 261257 23248 1\n",
      "60500000 0.396646022797 261688 763 1\n",
      "60600000 0.395990133286 262092 55128 1\n",
      "60700000 0.376102924347 262492 8584 1\n",
      "60800000 0.396896123886 262919 37492 1\n",
      "60900000 0.372148036957 263336 99077 1\n",
      "61000000 0.393916130066 263770 28356 1\n",
      "61100000 0.376796007156 264182 23512 1\n",
      "61200000 0.382465839386 264563 36130 1\n",
      "61300000 0.456706047058 264979 37012 3\n",
      "61400000 0.419604063034 265389 29761 1\n",
      "61500000 0.556946992874 265819 33640 1\n",
      "61600000 0.450572013855 266239 44181 4\n",
      "61700000 0.475183010101 266658 15900 5\n",
      "61800000 0.417816162109 267063 21309 1\n",
      "61900000 0.429587125778 267479 4056 1\n",
      "62000000 0.398711919785 267912 43981 2\n",
      "62100000 0.397988796234 268340 22442 1\n",
      "62200000 0.43107509613 268750 36949 1\n",
      "62300000 0.424875974655 269161 25896 1\n",
      "62400000 0.427317857742 269584 19558 1\n",
      "62500000 0.421253919601 270029 59817 1\n",
      "62600000 0.39146399498 270495 33674 1\n",
      "62700000 0.408044099808 270911 15716 3\n",
      "62800000 0.438146114349 271328 42385 1\n",
      "62900000 0.402471065521 271740 2452 1\n",
      "63000000 0.40837597847 272158 5320 8\n",
      "63100000 0.403324127197 272581 20560 2\n",
      "63200000 0.379364013672 273008 26848 1\n",
      "63300000 0.375555992126 273399 18566 2\n",
      "63400000 0.36698102951 273776 22095 1\n",
      "63500000 0.379398107529 274227 17792 1\n",
      "63600000 0.414865016937 274662 14554 1\n",
      "63700000 0.399488210678 275077 11862 1\n",
      "63800000 0.372431993484 275486 32623 1\n",
      "63900000 0.376292943954 275909 33737 1\n",
      "64000000 0.361623048782 276333 31948 1\n",
      "64100000 0.373047113419 276772 33342 1\n",
      "64200000 0.404803991318 277207 39896 2\n",
      "64300000 0.663928985596 277624 12628 1\n",
      "64400000 0.556364059448 278038 98418 4\n",
      "64500000 0.448736906052 278473 9790 1\n",
      "64600000 0.493807077408 278907 14626 1\n",
      "64700000 0.467288970947 279334 11403 1\n",
      "64800000 0.486668109894 279733 10621 1\n",
      "64900000 0.410964012146 280116 12019 1\n",
      "65000000 0.444983005524 280529 15527 1\n",
      "65100000 0.433703184128 280980 39976 1\n",
      "65200000 0.43008685112 281375 40434 1\n",
      "65300000 0.710635900497 281769 2211 2\n",
      "65400000 0.649733066559 282162 32842 1\n",
      "65500000 0.477036952972 282589 2365 1\n",
      "65600000 0.60308098793 282988 14282 1\n",
      "65700000 0.412707090378 283414 34430 1\n",
      "65800000 0.441136837006 283810 6888 1\n",
      "65900000 0.418236970901 284210 35965 1\n",
      "66000000 0.446745157242 284640 43826 5\n",
      "66100000 0.40473818779 285085 85106 1\n",
      "66200000 0.400348186493 285526 45427 1\n",
      "66300000 0.461992025375 285944 71583 1\n",
      "66400000 0.52439904213 286372 37667 1\n",
      "66500000 0.503453969955 286768 33939 1\n",
      "66600000 0.438498020172 287171 64827 1\n",
      "66700000 0.421712875366 287599 12751 1\n",
      "66800000 0.396206855774 288025 5074 1\n",
      "66900000 0.421655893326 288442 3054 1\n",
      "67000000 0.423768997192 288898 28025 1\n",
      "67100000 0.418698072433 289322 15443 1\n",
      "67200000 0.422894954681 289737 24307 1\n",
      "67300000 0.424105167389 290176 52561 1\n",
      "67400000 0.493163824081 290605 43238 2\n",
      "67500000 0.41508102417 291016 37238 1\n",
      "67600000 0.442029953003 291425 43489 1\n",
      "67700000 0.428831100464 291851 31691 1\n",
      "67800000 0.427452087402 292293 825 1\n",
      "67900000 0.425770998001 292748 19176 1\n",
      "68000000 0.413583040237 293158 15831 2\n",
      "68100000 0.44026684761 293567 44053 1\n",
      "68200000 0.429401159286 294026 3572 1\n",
      "68300000 0.427734851837 294453 14076 1\n",
      "68400000 0.429039955139 294904 43753 1\n",
      "68500000 0.412445068359 295310 18712 1\n",
      "68600000 0.416538000107 295717 20783 1\n",
      "68700000 0.407114028931 296136 10044 1\n",
      "68800000 0.415988922119 296555 14824 1\n",
      "68900000 0.399106025696 296967 33224 1\n",
      "69000000 0.394107818604 297375 28456 5\n",
      "69100000 0.451174020767 297760 30318 1\n",
      "69200000 0.411339044571 298166 4640 1\n",
      "69300000 0.386752128601 298590 33093 2\n",
      "69400000 0.416061878204 298962 29363 1\n",
      "69500000 0.455144166946 299302 43975 3\n",
      "69600000 0.430116176605 299685 34464 1\n"
     ]
    }
   ],
   "source": [
    "# Make nips (or new york times) dataset!\n",
    "input_matrix = \"datasets/nips/docword.nips.txt.gz\"\n",
    "full_vocab = \"datasets/nips/vocab.nips.txt\"\n",
    "# input_matrix = \"datasets/nyt/docword.nytimes.txt\"\n",
    "# full_vocab = \"datasets/nyt/vocab.nytimes.txt\"\n",
    "\n",
    "firstNdocs = 'all'\n",
    "# output_matrix_name = \"datasets/nyt/M_nytimes.%s.mat\" % (firstNdocs) # \"M_nips.full_docs.mat\"\n",
    "output_matrix_name = \"datasets/nips/M_nips.%s.mat\" % (firstNdocs)\n",
    "\n",
    "print \"opening file\"\n",
    "\n",
    "# with gzip.open(input_matrix, 'r') as infile:\n",
    "with open(input_matrix, 'r') as infile:\n",
    "    num_docs = int(infile.readline())\n",
    "    num_words = int(infile.readline())\n",
    "    nnz = int(infile.readline())\n",
    "\n",
    "    print \"number of docs = %s, number of words = %s, number nonzero = %s\" % (num_docs, num_words, nnz)\n",
    "\n",
    "    data = []        # counts\n",
    "    row = []         # row (document) indices\n",
    "    col = []         # column (word) indices\n",
    "\n",
    "    print \"constructing word-document matrix\"\n",
    "    t0 = time.time()\n",
    "    if firstNdocs == 'all':\n",
    "        for i, l in enumerate(infile):\n",
    "            d, w, v = (int(x) for x in l.split())\n",
    "            if not i % 100000:\n",
    "                print i, time.time() - t0, d, w, v\n",
    "                t0 = time.time()\n",
    "            col.append(d-1)\n",
    "            row.append(w-1)\n",
    "            data.append(v)\n",
    "    else:\n",
    "        for i, l in enumerate(infile):\n",
    "            d, w, v = (int(x) for x in l.split())\n",
    "            if not i % 100000:\n",
    "                print i, time.time() - t0, d, w, v\n",
    "                t0 = time.time()\n",
    "\n",
    "            if d == firstNdocs + 1:\n",
    "                break\n",
    "            col.append(d-1)\n",
    "            row.append(w-1)\n",
    "            data.append(v)\n",
    "            # M[w-1, d-1] = v\n",
    "    M = scipy.sparse.csr_matrix((data, (row, col)))\n",
    "\n",
    "scipy.io.savemat(output_matrix_name, {'M' : M}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncateData(matrixName, full_vocab, numDocs=None, cutoff = 1000):\n",
    "    M = scipy.io.loadmat(matrixName)['M']\n",
    "    output_vocab = full_vocab + \".trunc\"\n",
    "    \n",
    "#     randIndices = np.random.choice(range(M.shape[1]), numDocs, replace=False)\n",
    "\n",
    "    print \"old num words = %s, old num docs = %s\" % M.shape\n",
    "\n",
    "    table = dict()\n",
    "    numwords = 0\n",
    "    with open(full_vocab, 'r') as f:\n",
    "        for line in f:\n",
    "            table[line.rstrip()] = numwords\n",
    "            numwords += 1\n",
    "\n",
    "    remove_word = [False]*numwords\n",
    "\n",
    "    # Read in the stopwords\n",
    "    with open('datasets/stopwords.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.rstrip() in table:\n",
    "                remove_word[table[line.rstrip()]] = True\n",
    "\n",
    "#     M = M[:,randIndices].tocsr() # M.tocsr()\n",
    "#     M = M.tocsr()\n",
    "    if numDocs:\n",
    "        randIndices = np.random.choice(range(M.shape[1]), numDocs, replace=False)\n",
    "        M = M[:,randIndices].tocsr()\n",
    "    else:\n",
    "        M = M.tocsr()\n",
    "    \n",
    "    new_indptr = np.zeros(M.indptr.shape[0], dtype=np.int32)\n",
    "    new_indices = np.zeros(M.indices.shape[0], dtype=np.int32)\n",
    "    new_data = np.zeros(M.data.shape[0], dtype=np.float64)\n",
    "\n",
    "    indptr_counter = 1\n",
    "    data_counter = 0\n",
    "\n",
    "    for i in xrange(M.indptr.size - 1):\n",
    "\n",
    "        # if this is not a stopword\n",
    "        if not remove_word[i]:\n",
    "\n",
    "            # start and end indices for row i\n",
    "            start = M.indptr[i]\n",
    "            end = M.indptr[i + 1]\n",
    "\n",
    "            # if number of distinct documents that this word appears in is >= cutoff\n",
    "            if (end - start) >= cutoff:\n",
    "                new_indptr[indptr_counter] = new_indptr[indptr_counter-1] + end - start\n",
    "                new_data[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.data[start:end]\n",
    "                new_indices[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.indices[start:end]\n",
    "                indptr_counter += 1\n",
    "            else:\n",
    "                remove_word[i] = True\n",
    "\n",
    "    new_indptr = new_indptr[0:indptr_counter]\n",
    "    new_indices = new_indices[0:new_indptr[indptr_counter-1]]\n",
    "    new_data = new_data[0:new_indptr[indptr_counter-1]]\n",
    "\n",
    "    M = scipy.sparse.csr_matrix((new_data, new_indices, new_indptr))\n",
    "    Mtrunc = M.tocsc()\n",
    "\n",
    "    print \"new num words = %s, new num docs = %s\" % M.shape\n",
    "\n",
    "    # Output the new vocabulary\n",
    "    output = open(output_vocab, 'w')\n",
    "    row = 0\n",
    "    with open(full_vocab, 'r') as f:\n",
    "        for line in f:\n",
    "            if not remove_word[row]:\n",
    "                output.write(line)\n",
    "            row += 1\n",
    "    output.close()\n",
    "    \n",
    "    return Mtrunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    def __init__(self, seed=int(time.time()), top_words=10, new_dim=1000, eps=10e-7, anchor_thresh=100):\n",
    "        self.seed = seed\n",
    "        self.top_words = top_words\n",
    "        self.new_dim = new_dim\n",
    "        self.eps = eps\n",
    "        self.anchor_thresh = anchor_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old num words = 102660, old num docs = 300000\n",
      "new num words = 8367, new num docs = 50000\n"
     ]
    }
   ],
   "source": [
    "input_matrix = \"datasets/nips/M_nips.all.mat\"\n",
    "full_vocab = \"datasets/nips/vocab.nips.txt\"\n",
    "numDocs = 1500\n",
    "thresh = 100\n",
    "cutoff = 50\n",
    "\n",
    "# input_matrix = \"datasets/nyt/M_nytimes.all.mat\"\n",
    "# full_vocab = \"datasets/nyt/vocab.nytimes.txt\"\n",
    "# numDocs = 50000\n",
    "# thresh = 500 # Anchor words must be in more than this number of documents\n",
    "# cutoff = 200 # Words must be in at least this number of documents, otherwise they are pruned.\n",
    "\n",
    "Mtrunc = truncateData(input_matrix, full_vocab, numDocs=numDocs, cutoff=cutoff)\n",
    "params = Params(seed=100, anchor_thresh=thresh, top_words=6, eps=1e-5)\n",
    "params.dictionary_file = full_vocab + \".trunc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole algorithm in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering word-topic matrix and topic-topic matrix\n",
      "parameters are: vocabulary size = 8367, number of documents = 50000, number of topics = 100, and tolerance = 1e-05\n",
      "calculating word-word co-occurance matrix\n",
      "Q calculated in:  36.6643409729\n",
      "finding anchors\n",
      "anchors calculated in:  8.71388411522\n",
      "recovering topic-document matrix A\n",
      "A calculated in:  39.8705518246\n",
      "88.4953508377\n"
     ]
    }
   ],
   "source": [
    "K = 100 # Number of topics to learn\n",
    "t0 = time.time()\n",
    "A, anchors, Q = highLevel(Mtrunc.copy(), params, K)\n",
    "print time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the words most associated with each topic in order of descending importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each topic's anchor word followed by a list of keywords in order of descending importance.\n",
      "\n",
      "advisory : advisory premature advise error wines client nyt zzz_islamabad fort held \n",
      "zzz_karl_horwitz : zzz_karl_horwitz financial lifestyle zzz_new_york zzz_americas mail general sector miles zzz_los_angeles \n",
      "tonight : tonight zzz_boston_globe zzz_x_x_x spot zzz_new_york zzz_fla light court zzz_chicago development \n",
      "copy : copy fall zzz_diane question expected answer version killed schedule zzz_new_york \n",
      "test : test export function mark thread housed media student check paragraph \n",
      "file : file zzz_boston_globe zzz_new_york spot sport zzz_fla zzz_los_angeles notebook internet zzz_calif \n",
      "solo : con dice solo son mayor era director sin sector embargo \n",
      "zzz_portland : zzz_portland coach assistant season team player job head coaching coaches \n",
      "publication : publication premature wines nyt zzz_islamabad greenhouse zzz_lee substitute risen advise \n",
      "wire : wire mandatory finance running today zzz_x_x_x produced tomorrow export function \n",
      "shares : million shares company offering public debt billion stock initial expected \n",
      "fax : fax financial mail zzz_new_york lifestyle zzz_americas general zzz_los_angeles miles solar \n",
      "zzz_paris : zzz_paris financial lifestyle zzz_new_york mail zzz_americas general sector miles zzz_los_angeles \n",
      "dates : sales dates major economic claim home scheduled weekly listed order \n",
      "send : send client error point advise word held telegram worth fort \n",
      "inning : run inning hit game home pitch ball lead left homer \n",
      "guard : guard team game player season play word close schedule games \n",
      "zzz_tom_oder : information zzz_eastern sport daily commentary question business separate marked today \n",
      "zzz_yasser_arafat : palestinian zzz_israel zzz_israeli zzz_yasser_arafat peace israeli official leader israelis attack \n",
      "released : released client error word point close telegram worth held law \n",
      "favor : favor financial lifestyle zzz_new_york mail general zzz_americas zzz_los_angeles miles zzz_peru \n",
      "kill : kill today function produced running water tomorrow part film movie \n",
      "par : zzz_tiger_wood shot player round par play tournament tour win won \n",
      "zzz_enron : zzz_enron company companies business firm stock employees executive billion chief \n",
      "zzz_laker : point zzz_laker team game season play zzz_kobe_bryant games player zzz_o_neal \n",
      "pepper : cup minutes add oil tablespoon pepper water sauce teaspoon fat \n",
      "zzz_microsoft : zzz_microsoft company computer software system window companies case business product \n",
      "tables : democratic tables environmental turning room home show restaurant night place \n",
      "touchdown : game yard play season team touchdown quarterback games coach player \n",
      "zzz_nasdaq : percent stock market company companies quarter point analyst investor economy \n",
      "earlier : earlier spot zzz_boston_globe article company stated advise zzz_washington summer zzz_nfl \n",
      "ballot : election ballot vote voter votes zzz_florida court recount official campaign \n",
      "zzz_taliban : zzz_taliban zzz_afghanistan government afghan forces official military zzz_u_s troop war \n",
      "glasses : glasses raise sexy thick frames home room friend book find \n",
      "anthrax : anthrax official mail letter attack office building worker zzz_fbi found \n",
      "lap : race car driver lap racing track win point won run \n",
      "ages : book children ages boy list sales find web author ranking \n",
      "zzz_cnn : zzz_cnn network president executive company business chief executives job cable \n",
      "priest : priest church abuse bishop sexual official victim children ago member \n",
      "newspaper : newspaper question evening fall endit source zzz_calif group part center \n",
      "zzz_medicare : drug percent plan program cost care health bill zzz_medicare benefit \n",
      "org : www site web org sites information online visit telegram mail \n",
      "shower : air rain wind shower storm weather front high water northern \n",
      "breast : women cancer percent patient breast study doctor drug risk found \n",
      "brown : dog corp brown quick jump lazy list home partial water \n",
      "king : king game goal team play season games point player period \n",
      "dedicated : dedicated producer include seek site part million show group member \n",
      "zzz_beijing : zzz_china chinese official government zzz_beijing zzz_united_states zzz_taiwan zzz_u_s leader country \n",
      "toss : survivor series toss interesting meaning set play find word water \n",
      "zzz_al_gore : zzz_al_gore campaign president zzz_bush voter democratic presidential vice poll zzz_clinton \n",
      "zzz_met : zzz_met team season player games braves play game fan manager \n",
      "ranger : ranger season team game player play games manager league guy \n",
      "zzz_black : zzz_black percent black white women book student american history number \n",
      "zzz_dodger : zzz_dodger season games manager start won guy league play win \n",
      "zzz_ucla : team game season point player play coach zzz_ucla games win \n",
      "smoking : fund smoking money scratch accompanying study group found percent million \n",
      "religion : religion error correct religious morning zzz_god group book leader political \n",
      "wine : wine wines flavor zzz_california restaurant fruit red food collection grapes \n",
      "album : music song band album show rock record sound pop group \n",
      "noon : talk wall noon earth upcoming american room home fury hour \n",
      "zzz_cuba : zzz_cuba government zzz_miami zzz_united_states cuban official zzz_u_s father boy family \n",
      "missile : zzz_bush missile system defense administration zzz_clinton official nuclear zzz_united_states zzz_russia \n",
      "zzz_aid : drug zzz_aid million percent government patient countries billion zzz_hiv group \n",
      "medal : team zzz_olympic medal women won games gold win sport zzz_u_s \n",
      "zzz_christmas : zzz_christmas morning throw family home hour book friend holiday room \n",
      "chocolate : chocolate sugar food cream cup flavor egg buy milk butter \n",
      "zzz_cb : show network zzz_cb zzz_nbc television zzz_abc season series night hour \n",
      "zzz_india : zzz_india zzz_pakistan government group zzz_united_states official war country military zzz_indian \n",
      "surplus : tax billion cut zzz_bush government percent economy money spending fund \n",
      "gun : gun law police control bill show officer group shooting weapon \n",
      "zzz_giant : team season player zzz_giant game coach games play guy league \n",
      "referred : article referred governor zzz_new_york campaign zzz_new_hampshire president company defeated political \n",
      "zzz_ford : company car zzz_ford tires sales percent sport vehicles chief market \n",
      "yankees : yankees team season player million zzz_new_york won play baseball win \n",
      "zzz_oscar : film movie director actor play award character movies show zzz_oscar \n",
      "abortion : women abortion law zzz_bush campaign court group republican political bill \n",
      "zzz_russian : zzz_russian zzz_russia official zzz_vladimir_putin russian zzz_moscow government military station officer \n",
      "pension : plan fund company percent money million companies stock pension worker \n",
      "zzz_dick_cheney : zzz_bush president zzz_dick_cheney zzz_white_house administration republican campaign attack zzz_congress presidential \n",
      "defendant : case court lawyer trial law death prosecutor federal judge attorney \n",
      "gay : gay group member sex show political republican lesbian meeting part \n",
      "gallon : prices oil car water gas price fuel gallon cost power \n",
      "stem : research stem human scientist company zzz_bush patient president plant researcher \n",
      "elementary : school student teacher program children high parent public kid education \n",
      "purchases : percent company information companies customer sales market million business consumer \n",
      "flag : flag zzz_american black american attack white country zzz_america member political \n",
      "virus : virus computer program official system disease mail found bird human \n",
      "sanction : government trade official zzz_united_states sanction zzz_u_s zzz_iraq zzz_iran zzz_american administration \n",
      "grandchildren : zzz_new_york family son died home wife children daughter book father \n",
      "airlines : percent flight company passenger security airline business official companies airport \n",
      "horses : race won horse horses winner trainer million win racing stakes \n",
      "laden : laden bin attack terrorist official zzz_u_s zzz_united_states zzz_american zzz_bush terrorism \n",
      "cow : animal cow food disease human zzz_new_york plant found part product \n",
      "auction : million company percent auction site business market companies bill web \n",
      "zzz_mexico : zzz_mexico zzz_vicente_fox director mexican sector miles trade cafe local region \n",
      "telex : syndicate zzz_new_york www telex zzz_canada zzz_asia era http zzz_new_jersey hotel \n",
      "zzz_george_bush : zzz_george_bush president campaign zzz_john_mccain political republican election zzz_texas presidential zzz_republican \n",
      "privacy : web information companies site computer law mail privacy company zzz_internet \n",
      "leather : season women show designer black leather fashion car collection boy \n",
      "zzz_nato : war military zzz_nato government zzz_kosovo zzz_united_states troop leader zzz_u_s official \n"
     ]
    }
   ],
   "source": [
    "print \"Each topic's anchor word followed by a list of keywords in order of descending importance.\\n\"\n",
    "vocab = file(full_vocab + \".trunc\").read().strip().split()\n",
    "for k in xrange(K):\n",
    "    topwords = np.argsort(A[:, k])[-10:][::-1]\n",
    "    print vocab[anchors[k]], ':',\n",
    "    for w in topwords:\n",
    "        print vocab[w],\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the topic-topic correlation matrix to view potential correlations between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x109c67150>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAJeCAYAAAD82dIzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a63dd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "R = np.dot(np.linalg.pinv(A), np.dot(Q, np.linalg.pinv(A.T))) # Computing the topic-topic covariance matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pcolor(R, norm=None, cmap='Blues')\n",
    "plt.title(\"Topic-topic correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the parameters of $\\alpha$ for the topic-document Dirichlet distribution $\\tau.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcAlpha(R):\n",
    "    alphaOverAlpha0 = np.dot(R, np.ones(R.shape[0]))\n",
    "    i = alphaOverAlpha0.argmin()\n",
    "    u, v = R[i,i], alphaOverAlpha0[i]\n",
    "    alpha0 = (1 - (u/v))/((u/v) - v)\n",
    "    alpha = alpha0 * alphaOverAlpha0\n",
    "    return alpha, alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01482729,  0.01684305,  0.04064545,  0.0371535 ,  0.09473873,\n",
       "         0.03836407,  0.16079619,  0.10067844,  0.02738376,  0.06537202,\n",
       "         0.49560479,  0.04703361,  0.03167202,  0.26383021,  0.03998449,\n",
       "         0.53474516,  0.04023191,  0.40552972,  0.75414944,  0.05027042,\n",
       "         0.03312532,  0.17254458,  0.45826832,  0.57504078,  0.69894165,\n",
       "         0.49661029,  0.60142218,  0.37659544,  0.79724495,  1.11137475,\n",
       "         0.25841612,  0.66166829,  0.55120452,  0.37938664,  0.48810023,\n",
       "         0.51435209,  0.54211884,  0.36812331,  0.55316861,  0.19576816,\n",
       "         0.6076717 ,  0.43812427,  0.75566975,  0.58344115,  0.31518189,\n",
       "         0.75048077,  0.39136663,  0.58293235,  0.18629543,  0.52199163,\n",
       "         0.33368786,  0.49258965,  0.52855665,  0.28093796,  0.90778155,\n",
       "         0.40971158,  0.4640747 ,  0.41657754,  1.04263171,  0.24761961,\n",
       "         0.34118148,  0.41079399,  0.48217188,  0.67416518,  0.4514224 ,\n",
       "         0.34667813,  0.78944787,  0.53289068,  0.61997093,  0.64970896,\n",
       "         0.47919272,  0.4773617 ,  0.6837891 ,  0.40872409,  1.37016115,\n",
       "         0.61513208,  0.58168425,  0.70065155,  0.45906463,  1.3696742 ,\n",
       "         0.61091451,  0.76572838,  0.66870645,  1.51043102,  0.60618614,\n",
       "         0.57228746,  0.54505714,  0.48287676,  1.24425684,  1.24597427,\n",
       "         0.60623593,  0.43250997,  0.65849959,  0.92584321,  0.10687744,\n",
       "         0.0902769 ,  0.45024457,  0.99013477,  0.88618195,  0.59413717]),\n",
       " 49.548879569353623)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha, alpha0 = calcAlpha(R)\n",
    "alpha, alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Papers discussed:  \n",
    "**Arora et. al. [2012a]:** Arora, S., Ge, R., and Moitra, A. Learning topic models – going beyond svd. In FOCS, 2012b.  \n",
    "**Arora et. al. [2012b]:** Sanjeev Arora, Rong Ge, Yoni Halpern, David M. Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. 2013. A practical algorithm for topic modeling with provable guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
