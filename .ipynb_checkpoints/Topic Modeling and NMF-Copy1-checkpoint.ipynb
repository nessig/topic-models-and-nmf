{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more about ability to model correlated data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level motivation:\n",
    "Given a gigantic corpus of documents (e.g., 300,000 New York Times articles), how would you come up with a set of topics that could be mixed togeather to explain each document in your corpus? Maybe one document is a mixture of 70% about politics and 30% about health, but another document is 20% about politics, 30% about economics, and 50% about cooking. Being able to discover these topics could give deep insight into the existece of fundemental groupings within your data. Additionally, you could quickly create breif and high-level summaries of incomming (possibly large) documents by computing their topic mixtures. **Topic Modeling** is a method of automatic data comprehension and classification whereby latent topics are discovered within a corpus of documents, and used to label existing documents as well as new documents. Topic models are not restricted to news article type documents; models can be constructed from \"documents\" consisting of anything from web pages, to images, to ratings, to genetic sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our topic model\n",
    "In order to learn the topic structure, we must assume the existence of a topic structure from which our documents were generated. Therefore we let $V$ be the size of our vocabulary and let $K$ be the number of topics. We also assume a prior distribution $\\tau$ from which we generate topic distributions for each document, as well as a multinomial distribution $A_k$ over the vocabulary associated with each topic $k$. A document is represented as a vector of word frequencies (bag-of-words model). We model a document as a convex combination (or mixture) of $K$ topics, and topics as distributions over a vocabulary of $V$ words (i.e., topic vectors of word frequencies).   \n",
    "\n",
    "In this *mixture model* a document $d$ is generated by first selecting its distribution of topics (e.g., 80% politics, 20% economics, 0% cooking), which we denote $W_d \\sim \\tau.$ Then, for each word position $i$ in the document we chose a topic for that position $z_i \\sim W_i,$ and finally a word from that topic $w_i \\sim A_{z_i}.$  \n",
    "\n",
    "We may represent each word-topic distribution as a column $A_k$ in a $V \\times K$ word-topic matrix $A$, and similarly define the $K \\times N$ topic-document matrix in terms of the randomly generated topic distributions $W_d$ of our $N$ documents. In this representation we can interpret the $V$ dimensional vector $Av$, where $v \\in \\mathbb{R}^K \\text{ and } \\sum_i v_i = 1$, to be the distribution of word frequencies for a document with topic weights $v$. Taking this product with $A$ for an infinite number of documents $W_d$ gives us the $V \\times N$ matrix $M$ that encodes the expected word frequencies for $N$ documents.\n",
    "\n",
    "For particular topic models like Latent Dirichlet Allocation (LDA) where the prior disribution $\\tau$ over the topic distribution of a document is a Dirichlet distribution, we would like to learn the model's parameters. So, the learning problem for topic modeling is: given topic model how do we actually go about learning the matrix of word-topic distributions $A$ and the paramaters of $\\tau$?  \n",
    "\n",
    "Because MLE is NP-hard for more than one topic, typically people use MCMC algorithms to approximately infer the parameters (e.g., Gibbs sampling). The algorithm described in **Arora et. al. [2012b]** is a simple and highly performant alternative to methods like Gibbs sampling, and will be the focus of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arora's topic inference modeling algorithm\n",
    "In **Arora et. al. [2012a]** the authors prove there is a poly-time algorithm that learns the parameters of any topic model under the assumption that the word-topic matrix $A$ is *seperable*.\n",
    "\n",
    "### Seperability assumption\n",
    "A word-topic matrix $A$ is said to be $p$-seperable if there exists a $p > 0$ such that for all topics $k \\in [K]$ there exists a word $i$ that satisfies $A[i,k] > p$ and $A[i,k'] = 0$ for $k' \\neq k.$ These words are called *anchor words*, and are important because if we find one in a document then we know the document must at least partially be about the anchor word's corresponding topic, since the probability of that word being generated by any other topic is zero. An important observation is that if $A$ is sepreable then in the product $AW = M$ the rows of the matrix $W$ appear as scaled rows of $M$. This happens because for any given anchor word $i$ of $A$ corresponding to topic $k$, the row $A[i, :]$ has a single nonzero value $p_k$ at index $k$. Dotting $A[i, :]$ with $W$ to form row $i$ of $M$ we can see only row $W[k, :]$ appears scaled by $p_k$ in $M$, $M[i, :] = p_k * W[k, :].$ Given $M$, we can reconstruct $W$ using the rows of $M$, and then compute $A$ up to scaling.  \n",
    "\n",
    "### Overview of the algorithm\n",
    "The algorithm has two basic steps:  \n",
    "1) **anchor selection**, which finds the anchor words.  \n",
    "2) **recovery**, which recovers the matrix $A$ and the parameters of $\\tau$.  \n",
    "\n",
    "The input to both steps of the algorithm is the $V \\times V$ word-word co-occurance matrix $Q$, which is normalized to sum to $1$ over all its entries.  \n",
    "\n",
    "The `highLevel` function below implements the full algorithm, using helper functions to generate the input $Q$ for the find anchors stepand the recovery step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highLevel(M, params, K):\n",
    "    \"\"\"\n",
    "    Input: word by document sparse matrix of word counts M,\n",
    "            params including tolerance parameter eps > 0 and more,\n",
    "            number of anchors K.\n",
    "    Output: word-topic matrix A, topic-topic matrix R.\n",
    "    \"\"\"\n",
    "    print \"Recovering word-topic matrix and topic-topic matrix\"\n",
    "    print \"parameters are: vocabulary size = %s, number of documents = %s, number of topics = %s, and tolerance = %s\" \\\n",
    "                    % (M.shape[0], M.shape[1], K, params.eps)\n",
    "    candidates = getCandidates(M.tocsr(), params)\n",
    "    print \"calculating word-word co-occurance matrix\"\n",
    "    t0 = time.time()\n",
    "    Q = wordCooccurrence(M)\n",
    "    print \"Q calculated in: \", time.time() - t0\n",
    "    print \"finding anchors\"\n",
    "    t0 = time.time()\n",
    "    anchors = findAnchors(Q, K, params, candidates)\n",
    "    print \"anchors calculated in: \", time.time() - t0\n",
    "    print \"recovering topic-document matrix A\"\n",
    "    Q2 = Q.copy()\n",
    "    t0 = time.time()\n",
    "    A, iterations = recoverL2(Q, anchors, params.eps)\n",
    "    print \"A calculated in: \", time.time() - t0\n",
    "    return A, anchors, Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCandidates(M, params):\n",
    "    candidate_anchors = []\n",
    "    for i in xrange(M.shape[0]):\n",
    "        if M[i, :].nnz > params.anchor_thresh:\n",
    "            candidate_anchors.append(i)\n",
    "    return candidate_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a sparse CSC document matrix M (with floating point entries),\n",
    "# computes the word-word correlation matrix Q\n",
    "def wordCooccurrence(M):\n",
    "    vocabSize = M.shape[0]\n",
    "    numdocs = M.shape[1]\n",
    "    \n",
    "    diag_M = np.zeros(vocabSize)\n",
    "\n",
    "    for d in xrange(M.indptr.size - 1):\n",
    "        \n",
    "        # start and end indices for document d\n",
    "        start = M.indptr[d]\n",
    "        end = M.indptr[d + 1]\n",
    "        \n",
    "        nd = np.sum(M.data[start:end])\n",
    "        row_indices = M.indices[start:end]\n",
    "        \n",
    "        if nd*(nd-1) != 0:\n",
    "            diag_M[row_indices] += M.data[start:end]/(nd*(nd-1))\n",
    "            M.data[start:end] = M.data[start:end]/math.sqrt(nd*(nd-1))\n",
    "    \n",
    "    \n",
    "    Q = M*M.transpose()/numdocs\n",
    "    Q = Q.todense()\n",
    "    Q = np.array(Q, copy=False)\n",
    "\n",
    "    diag_M = diag_M/numdocs\n",
    "    Q = Q - np.diag(diag_M)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `wordCooccurance` we loop through each document to calculate `diag_M`, which is incremented by each documents word counts scaled by $n_d (n_d - 1)$, where $wpd$ is the total number of words in that document. After incrementing `diag_M` we then scale each documents counts by $\\sqrt{n_d (n_d - 1)}$. At the end of the loop on a document $d$ we could form the matrix `M.data[start:end] * M.data[start:end].T - np.diag(diag_M)`, which is equal to $\\frac{1}{n_d(n_d - 1)} \\sum_{i,j \\in [n_d], i \\neq j} e_{z_{d,i}} e_{z_{d,j}}^T$ where $z_{d,i}$ denotes the word in document $d$ at position $i$. Since the expected value of all terms $e_{z_{d,i}} e_{z_{d,j}}^T$ is just $(AW_d)(AW_d)^T$, we have that in expectation `M.data[start:end] * M.data[start:end].T - np.diag(diag_M)` is equal to $\\frac{1}{n_d(n_d - 1)}( \\mathbb{E}[M_dM_d^T] - \\mathbb{E}[\\mathrm{diag}(M_d)]) = \\frac{1}{n_d(n_d - 1)}( \\mathbb{E}[M_d]\\mathbb{E}[M_d]^T + \\mathrm{Cov}(M_d) - \\mathrm{diag}(\\mathbb{E}[M_d])) = A W_d W_d^T A^T$, conditioned on $W_d$. We calculate $Q$ by subtracting the diagonal matrix of our running scaled word counts from the appropiately scaled `M.data[start:end] * M.data[start:end].T` (cancels out the diagonal term in the covariance matrix, thereby making it the unbiased estimator we were looking for) and dividing by number of documents, which has the expectation  $\\frac{1}{N} \\sum_{d=1}^N (AW_d)(AW_d)^T$ conditioned on the $W_d$'s. Later on we will show how performing inference on the word-word co-occurance statistics dramatically speeds up our algorithm, allowing us to only need to pass through our corpus once when calculating $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given infinite documents, the convex hull of the rows of $\\bar{Q}$ will be a simplex with vertices corresponding to anchor words. Unfortunately, we don't have an infinite supply of documents, therefore the best we can do is approximate this set of vertices. Given $V$ points $d_1, \\dots, d_V$  that are each a perturbation of $a_1, \\dots, a_V$ whose convex hull forms a simplex $P$, our goal is to approximate the vertices of $P$.  \n",
    "\n",
    "**Main idea of algorithm:** on each iteration the algorithm finds the furthest point from the subspace spanned by the set of current \"anchors\" $S$. If you already found a few vertices that are each close to different anchor words, you want to look for new anchor words as far away from $\\mathbb{span}(S)$ as possible, since anchor words don't lie within  the span of any other vertices.  \n",
    "\n",
    "This algorithm also utilized dimentionality reduction through a random subspace projection (Achlioptas 2003). The stabalized Gram-Schmidt process is responsible for orthonormalizing the set of current anchors, against whose span all distances are being compared (evidently Gram-Schmidt isn't numerically stable and often results in nonorthogonality!).\n",
    "\n",
    "### findAnchors algorithm\n",
    "Input: $V$ points in $\\mathbb{R}^V$ almost in a simplex (corresponding to $\\bar{Q}$'s rows) with $K$ vertices and $\\epsilon > 0.$  \n",
    "Output: $K$ points that are close to the vertices of the simplex (i.e., approximate anchor words).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findAnchors(Q, K, params, candidates):\n",
    "    # Random number generator for generating dimension reduction\n",
    "    prng_W = RandomState(params.seed)\n",
    "\n",
    "    new_dim = params.new_dim\n",
    "\n",
    "    # row normalize Q\n",
    "    row_sums = Q.sum(1)\n",
    "    for i in xrange(len(Q[:, 0])):\n",
    "        Q[i, :] = Q[i, :]/float(row_sums[i])\n",
    "\n",
    "    # Reduced dimension random projection method for recovering anchor words\n",
    "    Q_red = Random_Projection(Q.T, new_dim, prng_W)\n",
    "    Q_red = Q_red.T\n",
    "    (anchors, anchor_indices) = Projection_Find(Q_red, K, candidates)\n",
    "\n",
    "    # restore the original Q\n",
    "    for i in xrange(len(Q[:, 0])):\n",
    "        Q[i, :] = Q[i, :]*float(row_sums[i])\n",
    "\n",
    "    return anchor_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Projection_Find(M_orig, r, candidates):\n",
    "\n",
    "    n = M_orig[:, 0].size\n",
    "    dim = M_orig[0, :].size\n",
    "\n",
    "    M = M_orig.copy()\n",
    "    \n",
    "    # stored recovered anchor words\n",
    "    anchor_words = np.zeros((r, dim))\n",
    "    anchor_indices = np.zeros(r, dtype=np.int)\n",
    "\n",
    "    # store the basis vectors of the subspace spanned by the anchor word vectors\n",
    "    basis = np.zeros((r-1, dim))\n",
    "\n",
    "\n",
    "    # find the farthest point p1 from the origin\n",
    "    max_dist = 0\n",
    "    #for i in range(0, n):\n",
    "    for i in candidates:\n",
    "        dist = np.dot(M[i], M[i])\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            anchor_words[0] = M_orig[i]\n",
    "            anchor_indices[0] = i\n",
    "\n",
    "    # let p1 be the origin of our coordinate system\n",
    "    #for i in range(0, n):\n",
    "    for i in candidates:\n",
    "        M[i] = M[i] - anchor_words[0]\n",
    "\n",
    "\n",
    "    # find the farthest point from p1\n",
    "    max_dist = 0\n",
    "    for i in candidates:\n",
    "        dist = np.dot(M[i], M[i])\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            anchor_words[1] = M_orig[i]\n",
    "            anchor_indices[1] = i\n",
    "            basis[0] = M[i]/np.sqrt(np.dot(M[i], M[i]))\n",
    "\n",
    "\n",
    "    # stabilized gram-schmidt which finds new anchor words to expand our subspace\n",
    "    for j in range(1, r - 1):\n",
    "\n",
    "        # project all the points onto our basis and find the farthest point\n",
    "        max_dist = 0\n",
    "        #for i in range(0, n):\n",
    "        for i in candidates:\n",
    "            M[i] = M[i] - np.dot(M[i], basis[j-1])*basis[j-1]\n",
    "            dist = np.dot(M[i], M[i])\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                anchor_words[j + 1] = M_orig[i]\n",
    "                anchor_indices[j + 1] = i\n",
    "                basis[j] = M[i]/np.sqrt(np.dot(M[i], M[i])) \n",
    "                \n",
    "    # convert numpy array to python list\n",
    "    anchor_indices_list = []\n",
    "    for i in range(r):\n",
    "        anchor_indices_list.append(anchor_indices[i])\n",
    "    \n",
    "    return (anchor_words, anchor_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Project the columns of the matrix M into the\n",
    "# lower dimension new_dim using Achlioptas 2003 method\n",
    "def Random_Projection(M, new_dim, prng):\n",
    "    # transformer = random_projection.SparseRandomProjection(new_dim)\n",
    "    # M_red = transformer.fit_transform(M)\n",
    "    old_dim = M[:, 0].size\n",
    "    p = np.array([1./6, 2./3, 1./6])\n",
    "    c = np.cumsum(p)\n",
    "    randdoubles = prng.random_sample(new_dim*old_dim)\n",
    "    R = np.searchsorted(c, randdoubles)\n",
    "    R = math.sqrt(3)*(R - 1)\n",
    "    R = np.reshape(R, (new_dim, old_dim))\n",
    "    \n",
    "    M_red = np.dot(R, M)\n",
    "    return M_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recoverL2 algorithm\n",
    "For two words in a document $w_1,w_2$ and their topic assignments $z_1,z_1$ we have the following identities:\n",
    "1. $A[i,k] = p(w_1 = i \\vert z_1 = k)$  \n",
    "2. $Q[i,j] = p(w_1 = i, w_2 = j)$  \n",
    "3. $\\bar{Q}[i,j] = p(w_2 = j \\vert w_1 = i)$  \n",
    "\n",
    "Let $S$ be the set of anchor word indexes. The convex hull formed by the rows of $\\bar{Q}$ indexed by elements of $S$ contains all other rows of $\\bar{Q}$. To see this, note that for anchor words $s_k$, we can show $$\\bar{Q}[s_k,j] = p(w_2 = j \\vert z_1 = k).$$ For all other words $i$, we also have $$\\bar{Q}[i,j] = \\sum_k p(z_1 = k \\vert w_1 = i) p(w_2 = j \\vert z_1 = k).$$ If we denote $p(z_1 = k \\vert w_1 = i)$ as the coefficent $\\alpha_{i,k}$ we can rewrite the above expression more compactly as $\\bar{Q}[i,j] = \\sum_k \\alpha[i,k] \\bar{Q}[s_k,j].$ Since $\\alpha$'s rows consist of non-negative probabilities that sum to one, we have that any row of $\\bar{Q}$ lies in the convex hull of the rows corresponding to the anchor words. The mixing weights of these convex combinations give us $p(z_1 \\vert w_1 = i).$ Using this togeather with $p(w_1 = i)$, we can recover $A$ by applying Bayes' rule: $$A[i,k] = p(w_1 \\vert z_1 = k) = \\frac{p(z_1 \\vert w_1 = i) p(w_1 = i)}{\\sum_{i'} p(z_1 = k | w_1 = i') p(w_1 = i')}.$$ We calculate $p(w_1 = i)$ in terms of $Q$ by noting the following equation $$\\sum_j Q[i,j] = \\sum_j p(w_1 = i, w_2 = j) = p(w_1 = i).$$ With all the terms finally computed, we recover $A$ via the application of Bayes' theorem above.  \n",
    "\n",
    "For each row of the empirical row normalized co-occurance matrix $\\hat{Q}$, `recoverL2` finds the coefficents $p(z_1 \\vert w_1 = i)$ that best reconstruct the row as a convex combination of rows corresponding to anchor words using the exponentiated gradient algorithm `quadSolveExpGrad`. Then by applying Bayes' theorem as before, `recoverL2` can recover $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recoverL2(Q, anchors, eps=10**(-7)):\n",
    "    \"\"\"\n",
    "    Input: Matrix Q, Set of words S, tolerance parameter eps\n",
    "    Output: Matrices A,R\n",
    "    \"\"\"\n",
    "    Qold = np.copy(Q)\n",
    "    V = Q.shape[0]\n",
    "    K = len(anchors)\n",
    "    A = np.matrix(np.zeros((V,K)))\n",
    "    iterations = []\n",
    "    \n",
    "    # Store the normalization constnts P_w = Q*ones\n",
    "    P_w = np.matrix(np.diag(np.dot(Q, np.ones(V))))\n",
    "    for v in xrange(V):\n",
    "        if np.isnan(P_w[v,v]):\n",
    "            P_w[v,v] = 10**(-16)\n",
    "    \n",
    "    #normalize the rows of Q_prime\n",
    "    for v in xrange(V):\n",
    "        Q[v,:] = Q[v,:]/Q[v,:].sum()\n",
    "            \n",
    "    X = Q[anchors, :]\n",
    "    XXT = np.dot(X, X.transpose())\n",
    "\n",
    "    for i in xrange(V):\n",
    "        y = Q[i, :]\n",
    "        alpha, it = recover(y,X,i,anchors,XXT,eps)\n",
    "        A[i, :] = alpha\n",
    "        iterations.append(it)\n",
    "    \n",
    "    #rescale A matrix\n",
    "    #Bayes rule says P(w|z) proportional to P(z|w)P(w)\n",
    "\n",
    "    A = P_w * A\n",
    "\n",
    "    #normalize columns of A. This is the normalization constant P(z)\n",
    "    colsums = A.sum(0)\n",
    "\n",
    "    for k in xrange(K):\n",
    "        A[:, k] = A[:, k]/A[:,k].sum()\n",
    "    \n",
    "    A = np.array(A)\n",
    "#     R = np.dot(np.linalg.pinv(A), np.dot(Qold, np.linalg.pinv(A).T))\n",
    "#     R = np.dot(A.conj().T, np.dot(Qold, A.conj()))\n",
    "\n",
    "    return A, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recover(y,x,i,anchors,XXT,eps):\n",
    "    K = len(anchors)\n",
    "    alpha = np.zeros(K)\n",
    "    gap = None\n",
    "    if i in anchors:\n",
    "        alpha[anchors.index(i)] = 1\n",
    "        it = -1\n",
    "        dist = 0\n",
    "        stepsize = 0\n",
    "\n",
    "    else:\n",
    "        # alpha, it, dist, stepsize, gap = quadSolveExpGrad(y, x, eps, None, XXT)\n",
    "        alpha, it = quadSolveExpGrad(y, x, eps, None, XXT)\n",
    "        if np.isnan(alpha).any():\n",
    "                alpha = np.ones(K)/K\n",
    "\n",
    "    return alpha, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadSolveExpGrad(y, X, eps, alpha=None, XX=None): \n",
    "    \"\"\"Exponentiated gradient method\n",
    "    Input: vector y, matrix X, tolerance parameter eps\n",
    "    non-negative normalized vector alpha close to alpha'\n",
    "    that minimizes the L2 distance between y and X*alpha\n",
    "    finds alpha within eps of the alpha' minimizing the objective function obj(y, Xa) = aXXa - 2*aXY + YY,\n",
    "    where aXXA = dot(dot(alpha, dot(x, x.transpose())), alpha.transpose())\n",
    "    and aXY and YY are defined similarly.\n",
    "    Output: non-negative normalized vector alpha close to alpha' that minimizes the L2 distance between y and X*alpha,\n",
    "    number of iterations it,\n",
    "    final value of the objective function new_obj,\n",
    "    amount to change alpha by stepsize,\n",
    "    closeness to minimal alpha solution gap\"\"\"\n",
    "    \n",
    "    c1 = 10**(-4)\n",
    "    c2 = 0.75\n",
    "    if XX is None:\n",
    "        print 'making XXT'\n",
    "        XX = np.dot(X, X.transpose())\n",
    "\n",
    "    XY = np.dot(X, y)\n",
    "    YY = float(np.dot(y, y))\n",
    "\n",
    "\n",
    "    (K,n) = X.shape\n",
    "#     if alpha is None:\n",
    "#         alpha = np.ones(K)/K\n",
    "    alpha = np.ones(K)/K\n",
    "\n",
    "    old_alpha = np.copy(alpha)\n",
    "    log_alpha = np.log(alpha)\n",
    "    old_log_alpha = np.copy(log_alpha)\n",
    "\n",
    "    it = 1 \n",
    "    aXX = np.dot(alpha, XX)\n",
    "    aXY = float(np.dot(alpha, XY))\n",
    "    aXXa = float(np.dot(aXX, alpha.transpose()))\n",
    "\n",
    "    grad = 2*(aXX-XY)\n",
    "    new_obj = aXXa - 2*aXY + YY\n",
    "\n",
    "    old_grad = np.copy(grad)\n",
    "\n",
    "    stepsize = 1\n",
    "    repeat = False\n",
    "    decreased = False\n",
    "    gap = float('inf')\n",
    "\n",
    "    while 1:\n",
    "        eta = stepsize\n",
    "        old_obj = new_obj\n",
    "        old_alpha = np.copy(alpha)\n",
    "        old_log_alpha = np.copy(log_alpha)\n",
    "\n",
    "        if new_obj == 0 or stepsize == 0:\n",
    "            break\n",
    "\n",
    "#         if it % 1000 == 0:\n",
    "#             print \"\\titer\", it, new_obj, gap, stepsize\n",
    "        it += 1\n",
    "        #update\n",
    "        log_alpha -= eta*grad\n",
    "        #normalize\n",
    "        log_alpha -= logsum_exp(log_alpha)\n",
    "        #compute new objective\n",
    "        alpha = np.exp(log_alpha)\n",
    "\n",
    "        aXX = np.dot(alpha, XX)\n",
    "        aXY = float(np.dot(alpha, XY))\n",
    "        aXXa = float(np.dot(aXX, alpha.transpose()))\n",
    "\n",
    "        old_obj = new_obj\n",
    "        new_obj = aXXa - 2*aXY + YY\n",
    "#         old_obj, new_obj = new_obj, aXXa - 2 * aXY + YY\n",
    "        if not new_obj <= old_obj + c1*stepsize*np.dot(grad, alpha - old_alpha): #sufficient decrease\n",
    "            stepsize /= 2.0 #reduce stepsize\n",
    "            alpha = old_alpha \n",
    "            log_alpha = old_log_alpha\n",
    "            new_obj = old_obj\n",
    "            repeat = True\n",
    "            decreased = True\n",
    "            continue\n",
    "\n",
    "        #compute the new gradient\n",
    "        old_grad = np.copy(grad)\n",
    "        grad = 2*(aXX-XY)\n",
    "#         old_obj, new_obj = new_obj, aXXa - 2 * aXY + YY\n",
    "        \n",
    "        if (not np.dot(grad, alpha - old_alpha) >= c2*np.dot(old_grad, alpha-old_alpha)) and (not decreased): #curvature\n",
    "            stepsize *= 2.0 #increase stepsize\n",
    "            alpha = old_alpha\n",
    "            log_alpha = old_log_alpha\n",
    "            grad = old_grad\n",
    "            new_obj = old_obj\n",
    "            repeat = True\n",
    "            continue\n",
    "\n",
    "        decreased = False\n",
    "\n",
    "        lam = np.copy(grad)\n",
    "        lam -= lam.min()\n",
    "        \n",
    "        gap = np.dot(alpha, lam)\n",
    "        convergence = gap\n",
    "        if (convergence < eps):\n",
    "            break\n",
    "\n",
    "    return alpha, it #, it, new_obj, stepsize, gap\n",
    "\n",
    "def logsum_exp(y):\n",
    "    m = y.max()\n",
    "    return m + np.log((np.exp(y - m)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import wget\n",
    "import gzip\n",
    "#import scipy\n",
    "# from sklearn import random_projection\n",
    "import time\n",
    "import math\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# matplotlib.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have any datasets for topic modeling, here are a couple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nips.txt\"\n",
    "docwordUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nips.txt.gz\"\n",
    "wget.download(vocabUrl)\n",
    "wget.download(docwordUrl)\n",
    "\n",
    "# NY Times articles dataset (~300,000 documents, ~100,00 vocabulary, ~100,000,000 words)\n",
    "# vocabUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/vocab.nytimes.txt\"\n",
    "# docwordUrl = \"http://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/docword.nytimes.txt.gz\"\n",
    "# wget.download(vocabUrl)\n",
    "# wget.download(docwordUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the dataset into a sparse array below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening file\n",
      "number of docs = 1500, number of words = 12419, number nonzero = 746316\n",
      "constructing word-document matrix\n",
      "0 2.88486480713e-05 1 2 1\n",
      "100000 0.671155929565 205 1482 2\n",
      "200000 0.668588876724 410 4180 5\n",
      "300000 0.659167051315 612 5784 1\n",
      "400000 0.797510147095 814 2056 1\n",
      "500000 0.660013198853 1014 12130 62\n",
      "600000 0.733961105347 1219 7949 4\n",
      "700000 0.745787143707 1415 8809 1\n"
     ]
    }
   ],
   "source": [
    "# Make nips (or new york times) dataset!\n",
    "input_matrix = \"datasets/nips/docword.nips.txt.gz\"\n",
    "full_vocab = \"datasets/nips/vocab.nips.txt\"\n",
    "firstNdocs = 'all'\n",
    "output_matrix_name = \"datasets/nips/M_nips.%s.mat\" % (firstNdocs) # \"M_nips.full_docs.mat\"\n",
    "\n",
    "print \"opening file\"\n",
    "\n",
    "with gzip.open(input_matrix, 'r') as infile:\n",
    "    num_docs = int(infile.readline())\n",
    "    num_words = int(infile.readline())\n",
    "    nnz = int(infile.readline())\n",
    "\n",
    "    print \"number of docs = %s, number of words = %s, number nonzero = %s\" % (num_docs, num_words, nnz)\n",
    "\n",
    "    data = []        # counts\n",
    "    row = []         # row (document) indices\n",
    "    col = []         # column (word) indices\n",
    "\n",
    "    print \"constructing word-document matrix\"\n",
    "    t0 = time.time()\n",
    "    if firstNdocs == 'all':\n",
    "        for i, l in enumerate(infile):\n",
    "            d, w, v = (int(x) for x in l.split())\n",
    "            if not i % 100000:\n",
    "                print i, time.time() - t0, d, w, v\n",
    "                t0 = time.time()\n",
    "            col.append(d-1)\n",
    "            row.append(w-1)\n",
    "            data.append(v)\n",
    "    else:\n",
    "        for i, l in enumerate(infile):\n",
    "            d, w, v = (int(x) for x in l.split())\n",
    "            if not i % 100000:\n",
    "                print i, time.time() - t0, d, w, v\n",
    "                t0 = time.time()\n",
    "\n",
    "            if d == firstNdocs + 1:\n",
    "                break\n",
    "            col.append(d-1)\n",
    "            row.append(w-1)\n",
    "            data.append(v)\n",
    "            # M[w-1, d-1] = v\n",
    "    M = scipy.sparse.csr_matrix((data, (row, col)))\n",
    "\n",
    "scipy.io.savemat(output_matrix_name, {'M' : M}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truncateData(matrixName, full_vocab, numDocs, cutoff = 1000):\n",
    "    M = scipy.io.loadmat(matrixName)['M']\n",
    "    output_vocab = full_vocab + \".trunc\"\n",
    "    \n",
    "#     randIndices = np.random.choice(range(M.shape[1]), numDocs, replace=False)\n",
    "\n",
    "    print \"old num words = %s, old num docs = %s\" % M.shape\n",
    "\n",
    "    table = dict()\n",
    "    numwords = 0\n",
    "    with open(full_vocab, 'r') as f:\n",
    "        for line in f:\n",
    "            table[line.rstrip()] = numwords\n",
    "            numwords += 1\n",
    "\n",
    "    remove_word = [False]*numwords\n",
    "\n",
    "    # Read in the stopwords\n",
    "    with open('datasets/stopwords.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.rstrip() in table:\n",
    "                remove_word[table[line.rstrip()]] = True\n",
    "\n",
    "#     M = M[:,randIndices].tocsr() # M.tocsr()\n",
    "    M = M.tocsr()\n",
    "    \n",
    "    new_indptr = np.zeros(M.indptr.shape[0], dtype=np.int32)\n",
    "    new_indices = np.zeros(M.indices.shape[0], dtype=np.int32)\n",
    "    new_data = np.zeros(M.data.shape[0], dtype=np.float64)\n",
    "\n",
    "    indptr_counter = 1\n",
    "    data_counter = 0\n",
    "\n",
    "    for i in xrange(M.indptr.size - 1):\n",
    "\n",
    "        # if this is not a stopword\n",
    "        if not remove_word[i]:\n",
    "\n",
    "            # start and end indices for row i\n",
    "            start = M.indptr[i]\n",
    "            end = M.indptr[i + 1]\n",
    "\n",
    "            # if number of distinct documents that this word appears in is >= cutoff\n",
    "            if (end - start) >= cutoff:\n",
    "                new_indptr[indptr_counter] = new_indptr[indptr_counter-1] + end - start\n",
    "                new_data[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.data[start:end]\n",
    "                new_indices[new_indptr[indptr_counter-1]:new_indptr[indptr_counter]] = M.indices[start:end]\n",
    "                indptr_counter += 1\n",
    "            else:\n",
    "                remove_word[i] = True\n",
    "\n",
    "    new_indptr = new_indptr[0:indptr_counter]\n",
    "    new_indices = new_indices[0:new_indptr[indptr_counter-1]]\n",
    "    new_data = new_data[0:new_indptr[indptr_counter-1]]\n",
    "\n",
    "    M = scipy.sparse.csr_matrix((new_data, new_indices, new_indptr))\n",
    "    Mtrunc = M.tocsc()\n",
    "\n",
    "    print \"new num words = %s, new num docs = %s\" % M.shape\n",
    "\n",
    "    # Output the new vocabulary\n",
    "    output = open(output_vocab, 'w')\n",
    "    row = 0\n",
    "    with open(full_vocab, 'r') as f:\n",
    "        for line in f:\n",
    "            if not remove_word[row]:\n",
    "                output.write(line)\n",
    "            row += 1\n",
    "    output.close()\n",
    "    \n",
    "    return Mtrunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    def __init__(self, seed=int(time.time()), top_words=10, new_dim=1000, eps=10e-7, anchor_thresh=100):\n",
    "        self.seed = seed\n",
    "        self.top_words = top_words\n",
    "        self.new_dim = new_dim\n",
    "        self.eps = eps\n",
    "        self.anchor_thresh = anchor_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old num words = 12419, old num docs = 1500\n",
      "new num words = 2940, new num docs = 1500\n"
     ]
    }
   ],
   "source": [
    "input_matrix = \"datasets/nips/M_nips.all.mat\"\n",
    "full_vocab = \"datasets/nips/vocab.nips.txt\"\n",
    "numDocs = 1500\n",
    "thresh = 100\n",
    "cutoff = 50\n",
    "\n",
    "# input_matrix = \"M_nytimes.all.mat\"\n",
    "# full_vocab = \"vocab.nytimes.txt\"\n",
    "# numDocs = 50000\n",
    "# thresh = 100 # Anchor words must be in more than this number of documents\n",
    "# cutoff = 200 # Words must be in at least this number of documents, otherwise they are pruned.\n",
    "\n",
    "Mtrunc = truncateData(input_matrix, full_vocab, numDocs, cutoff=cutoff)\n",
    "params = Params(seed=100, anchor_thresh=thresh, top_words=6, eps=1e-7)\n",
    "params.dictionary_file = full_vocab + \".trunc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole algorithm in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovering word-topic matrix and topic-topic matrix\n",
      "parameters are: vocabulary size = 2940, number of documents = 1500, number of topics = 100, and tolerance = 1e-07\n",
      "calculating word-word co-occurance matrix\n",
      "Q calculated in:  1.88064694405\n",
      "finding anchors\n",
      "anchors calculated in:  2.29178214073\n",
      "recovering topic-document matrix A\n",
      "A calculated in:  63.9125750065\n",
      "68.4732167721\n"
     ]
    }
   ],
   "source": [
    "K = 100 # Number of topics to learn\n",
    "t0 = time.time()\n",
    "A, anchors, Q = highLevel(Mtrunc.copy(), params, K)\n",
    "print time.time() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the words most associated with each topic in order of descending importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each topic's anchor word followed by a list of keywords in order of descending importance.\n",
      "\n",
      "neuroscience : neuroscience application implementation navigation planning algorithm features feature hierarchical data \n",
      "part : part algorithm signal application implementation classification sequences vowel sequence digital \n",
      "iii : iii field statistical theorem lines point realization stimulus term simulation \n",
      "cognitive : cognitive representation activation unit university connectionist structure computer action level \n",
      "paul : david michael john richard peter thomas paul eric author andrew \n",
      "science : science level representation structure category subject constraint brain contribution left \n",
      "architecture : architecture algorithm problem step gradient convergence update local propagation descent \n",
      "theory : theory ill peak effect bar element region pair scale count \n",
      "visual : visual signal object image spatial level information representation scene neuronal \n",
      "processing : processing signal object information video power frames configuration pathway multiple \n",
      "policy : policy action algorithm function optimal reinforcement problem step states reward \n",
      "head : head direction velocity gain system input angular firing simulation neural \n",
      "speaker : speaker recognition network training neural performance system acoustic feature features \n",
      "spike : spike neuron firing rate spikes train input stimulus noise potential \n",
      "character : character network recognition system neural input training net layer output \n",
      "motion : motion direction velocity moving stage order filter unit detection flow \n",
      "speech : speech signal input output system recognition acoustic layer parameter filter \n",
      "face : face image images recognition faces set representation detection task pixel \n",
      "chip : chip analog circuit system neural output vlsi weight current digital \n",
      "classifier : classifier training classification set pattern class error problem performance data \n",
      "control : control planning navigation optimal memory real plan process programming white \n",
      "boolean : function threshold weight neural input size result number boolean network \n",
      "teacher : error weight training generalization unit hidden teacher noise input student \n",
      "missing : data set gaussian missing training input density variables likelihood distribution \n",
      "sound : sound auditory system frequency signal localization temporal spectral phase filter \n",
      "controller : system controller neural forward feedback function nonlinear error adaptive dynamic \n",
      "view : object view recognition unit representation image layer images features human \n",
      "kernel : kernel data function method space regression gaussian vector algorithm noise \n",
      "word : word recognition level phoneme training hmm experiment human result activation \n",
      "orientation : orientation cortex pattern cortical map contrast center spatial stimulus region \n",
      "expert : expert network algorithm task problem mixture output gating parameter local \n",
      "batch : algorithm error rate convergence batch gradient weight stochastic equation parameter \n",
      "motor : motor movement system command sensory arm feedback neuron internal position \n",
      "clustering : data cluster clustering algorithm set likelihood number point parameter problem \n",
      "winner : circuit input current output winner neuron analog voltage transistor gate \n",
      "hopfield : network neural neuron system hopfield problem dynamic equation graph solution \n",
      "channel : channel noise signal call frequency current information rate voltage power \n",
      "convex : function vector solution point algorithm problem convex set case constraint \n",
      "member : method feature member performance set features group selection error problem \n",
      "rules : rules rule network neural input set system unit weight output \n",
      "letter : unit letter hidden layer network recognition set segmentation segment features \n",
      "user : system user neural data set hand training performance detection test \n",
      "carlo : method distribution carlo gaussian monte algorithm bayesian parameter prior sampling \n",
      "penalty : weight term problem error method set training function data algorithm \n",
      "robot : robot task action reinforcement environment position goal space map learn \n",
      "contour : image contour direction images local point map pixel region segment \n",
      "operator : operator function system neural problem equation linear vector result matrix \n",
      "synapses : neuron synaptic synapses input pattern function synapse activity excitatory inhibitory \n",
      "loss : function loss algorithm vector bound parameter linear gradient case regression \n",
      "rbf : network rbf function training error neural output basis center input \n",
      "tree : tree node trees nodes decision algorithm probability structure data graph \n",
      "rotation : rotation digit pattern group transformation image translation plane lie angle \n",
      "separation : component separation matrix source sources independent gradient signal function algorithm \n",
      "concept : concept examples probability hypothesis class space bound positive sample number \n",
      "taylor : index author taylor van moody morgan lin solla william kearn \n",
      "light : light response potential result order intensity subject effect stimulus trial \n",
      "ensemble : network training error set ensemble neural data generalization test examples \n",
      "string : string set training symbol order language algorithm states length grammar \n",
      "mutual : information mutual distribution component joint features variable data variables conditional \n",
      "extra : input output net features task layer extra feature noise training \n",
      "interpolation : point interpolation function data space dimensional set linear manifold surface \n",
      "trajectory : trajectory point dynamic trajectories function optimal arm local minimum problem \n",
      "oscillation : neuron system phase dynamic oscillation oscillator parameter activity connection point \n",
      "eye : eye position system movement map target activity velocity retinal location \n",
      "content : pattern content style representation vector information filter number fig represent \n",
      "building : block building experiment level structure terminal basic complex address high \n",
      "module : module modules network system input object pattern modular memory hybrid \n",
      "matching : matching point graph algorithm image line level match constraint segment \n",
      "inverse : inverse mapping forward space output method matrix field joint hand \n",
      "processor : network neural weight layer processor net parallel bit performance nodes \n",
      "codes : code vector unit codes bit error information probability decoding coding \n",
      "attention : attention pattern map feature task recognition features selective location information \n",
      "subspace : vector algorithm tangent component matrix distance linear subspace parameter point \n",
      "capacity : pattern capacity memory network number result unit weight error performance \n",
      "tractable : distribution parameter field gaussian probability bound variables approximation likelihood log \n",
      "weighting : function data weighting weight linear variance equation parameter nonlinear factor \n",
      "utility : system result values problem decision utility step number performance method \n",
      "perceptton : weight network perceptron training unit input output pattern layer set \n",
      "sensor : sensor image field images noise system component current vision flow \n",
      "predictor : prediction error data neural training set predictor method series regression \n",
      "retina : retina response intensity level pixel circuit contrast current image center \n",
      "display : target task display parallel search result serial values information single \n",
      "weak : weak potential strong hypothesis strength experiment current phase combination combined \n",
      "regular : regular grammar markov source probability case parameter finite order log \n",
      "evidence : evidence data prior bayesian posterior parameter gaussian approximation condition result \n",
      "moment : neural equation function approximation order neuron stochastic dynamic moment transition \n",
      "context : context system hmm recognition dependent class information classes sequence approach \n",
      "adaptation : adaptation feedback current gain response rate adaptive circuit input contrast \n",
      "entropy : entropy distribution hidden algorithm function density gaussian representation approximation parameter \n",
      "ann : network neural set training ann data output result performance number \n",
      "grid : grid problem optimal method algorithm step cost local equation solution \n",
      "distances : distance point distances space set result spatial temporal structure correlation \n",
      "receptive : field receptive input layer unit function response connection stimulus spatial \n",
      "recurrent : network unit input neural output weight recurrent hidden layer net \n",
      "polynomial : function polynomial unit approximation error number bound network set theorem \n",
      "organizing : vector map input space organizing feature kohonen pattern weight mapping \n",
      "miller : miller correlation van index eigenvector matrix author term analysis john \n",
      "working : set problem vector data training point support working test result \n",
      "similarity : similarity representation memory features human structure similar product subject structural \n",
      "place : place field environment system location direction goal rate representation path \n"
     ]
    }
   ],
   "source": [
    "print \"Each topic's anchor word followed by a list of keywords in order of descending importance.\\n\"\n",
    "vocab = file(full_vocab + \".trunc\").read().strip().split()\n",
    "for k in xrange(K):\n",
    "    topwords = np.argsort(A[:, k])[-10:][::-1]\n",
    "    print vocab[anchors[k]], ':',\n",
    "    for w in topwords:\n",
    "        print vocab[w],\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and plot the topic-topic correlation matrix to view potential correlations between topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10eabbc50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAJeCAYAAAD82dIzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ea9b350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "\n",
    "R = np.dot(np.linalg.pinv(A), np.dot(Q, np.linalg.pinv(A.T))) # Computing the topic-topic covariance matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pcolor(R, norm=None, cmap='Blues')\n",
    "plt.title(\"Topic-topic correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For particular topic models like Latent Dirichlet Allocation (LDA) where the prior disribution $\\tau$ over the topic distribution of a document is a Dirichlet distribution, we would like to learn the model's parameters. So, for a given topic model how do we actually go about learning the paramaters of the matrix of word-topic distributions $A$ and of $\\tau$? Because MLE is NP-hard for more than one topic, typically people use MCMC algorithms to approximately infer the parameters (e.g., Gibbs sampling).  \n",
    "\n",
    "The algorithm described in **Arora et. al. [2012b]** is a simple and highly performant alternative to methods like Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random topic for each position in document:\n",
      "politics politics politics politics politics politics politics politics politics politics \n",
      "\n",
      "randomly generated document's words:\n",
      "Obama state state state election state election state election state\n"
     ]
    }
   ],
   "source": [
    "vocabulary = [\"Obama\", \"state\", \"election\", \"bank\", \"money\", \"401k\", \"bake\", \"oven\", \"cake\"]\n",
    "topics = {\"politics\": 0, \"economics\": 1, \"cooking\": 2}\n",
    "\n",
    "# Associate certain words with topics\n",
    "politicalWords = [\"Obama\", \"state\", \"election\"]\n",
    "economicWords = [\"bank\", \"money\", \"401k\"]\n",
    "cookingWords = [\"bake\", \"oven\", \"cake\"]\n",
    "\n",
    "V = len(vocabulary)\n",
    "K = len(topics)\n",
    "\n",
    "# Construct a word-topic distribution\n",
    "# Note: this choice assumes all words associated with a topic have equal probability of occuring\n",
    "A = np.zeros((V,K))\n",
    "for i, word in enumerate(vocabulary):\n",
    "    if word in politicalWords:\n",
    "        A[i, topics[\"politics\"]] = 1\n",
    "    if word in economicWords:\n",
    "        A[i, topics[\"economics\"]] = 1\n",
    "    if word in cookingWords:\n",
    "        A[i, topics[\"cooking\"]] = 1\n",
    "\n",
    "# Normalize the word-topic distributions\n",
    "for topic in topics.values():\n",
    "    A[:, topic] /= A[:, topic].sum()\n",
    "\n",
    "# Generate a document given its topic weights (topicWeights), and the word-topic distributions (A)\n",
    "numWords = 10\n",
    "document = []\n",
    "topicWeights = {\"politics\": 0.8, \"economics\": 0.2, \"cooking\": 0.0}\n",
    "# Assign each position in the document a topic\n",
    "documentTopics = np.random.choice(topicWeights.keys(), numWords, p=topicWeights.values())\n",
    "print \"random topic for each position in document:\\n\", \" \".join(documentTopics), \"\\n\"\n",
    "\n",
    "# For each topic, generate a word from the topic's associated word distribution\n",
    "for topic in documentTopics:\n",
    "    topicId = topics[topic]\n",
    "    wordTopicDistribution = A[:, topicId]\n",
    "    word = np.random.choice(vocabulary, 1, p=wordTopicDistribution)[0]\n",
    "    document.append(word)\n",
    "print \"randomly generated document's words:\\n\", \" \".join(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling using NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Arora et. al. [2012a]** the authors justify *Nonnegative Matrix Factorization* (NMF) as the \"correct\" way to do topic modeling, as opposed to existing approaches that rely on SVD. SVD approaches either make the assumption that a document only contains one topic, or else they can only recover the span of the topic vectors and not the actual topic vectors. The singular vectors of the SVD of a word-document matrix may contain negative values, which make interpreting them as topics very difficult if we wish to use for topics the most common measures of document similarity, such as the dot product. The SVD topic vectors are also orthonormal, which is not a property of real topics (the sets of words associated with the topics of economics and government are obviously not disjoint).  \n",
    "\n",
    "The NMF approach is the first poly-time algorithm that does not suffer from these limitations, although it does make an assumption about the *seperability* of the word-topic matrix.  \n",
    "\n",
    "The authors phrase the problem of topic modeling in the following way (**Arora et. al. [2012a]**):\n",
    ">**Problem:** There is an unknown topic matrix $A$ with\n",
    "nonnegative entries that is dimension $n \\times r$, and a stochastically generated unknown\n",
    "matrix $W$ that is dimension $r \\times m$. Each column of $A W$ is viewed as a probability\n",
    "distribution on rows, and for each column we are given $N \\ll n$ i.i.d. samples from the\n",
    "associated distribution.  \n",
    "**Goal**: Reconstruct $A$ and parameters of the generating distribution for $W$.\n",
    "\n",
    "Their paper focuses on acheiving this goal given only a small number of samples of the each column of $AW$'s distribution using NMF. In the context of topic modeling, they provide an algorithm for learning the word-topic matrix $A$ and the parameters of $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
